{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farrokhlabib/Documents/github/csAE/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from signals import *\n",
    "from frequencyestimator import *\n",
    "from scipy.optimize import basinhopping, minimize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "sns.set_context(\"poster\", font_scale = .45, rc={\"grid.linewidth\": 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = lambda n, theta: np.cos((2*n+1)*theta)**2\n",
    "P1 = lambda n, theta: np.sin((2*n+1)*theta)**2\n",
    "\n",
    "def estimate_signal(depths, n_samples, theta):\n",
    "        signals = np.zeros(len(depths), dtype = np.complex128)\n",
    "        measurements = np.zeros(len(depths))\n",
    "        for i,n in enumerate(depths):\n",
    "            # Get the exact measuremnt probabilities\n",
    "            p0 = P0(n, theta)\n",
    "            p1 = P1(n, theta)\n",
    "\n",
    "            p0x = P0x(n,theta)\n",
    "            p1x = P1x(n,theta)\n",
    "            \n",
    "            p0_estimate = np.random.binomial(n_samples[i], p0)/n_samples[i]\n",
    "            p1_estimate = 1 - p0_estimate\n",
    "\n",
    "            p0x_estimate = np.random.binomial(n_samples[i], p0x)/n_samples[i]\n",
    "            p1x_estimate = 1.0 - p0x_estimate\n",
    "            measurements[i] = p0_estimate\n",
    "            \n",
    "            # Estimate theta\n",
    "            theta_estimated = np.arctan2(p0x_estimate - p1x_estimate, p0_estimate - p1_estimate)\n",
    "\n",
    "            # Compute f(n) - Eq. 3\n",
    "            fi_estimate = np.exp(1.0j*theta_estimated)\n",
    "            signals[i] = fi_estimate\n",
    "         \n",
    "        return signals, measurements\n",
    "\n",
    "from scipy.stats import binom\n",
    "\n",
    "def apply_correction(ula_signal, measurements, theta_est, theta):\n",
    "    p_exact = np.cos((2*ula_signal.depths+1)*(theta))**2\n",
    "    p_neg = np.cos((2*ula_signal.depths+1)*(-theta))**2\n",
    "    p_o2 = np.cos((2 * ula_signal.depths + 1) * (theta_est/2.0)) ** 2\n",
    "    p_o4 = np.cos((2 * ula_signal.depths + 1) * (theta_est/4.0)) ** 2\n",
    "    p_same = np.cos((2*ula_signal.depths+1)*(theta_est))**2\n",
    "    p_s2 = np.cos((2 * ula_signal.depths + 1) * (np.pi/2-theta_est)) ** 2\n",
    "    p_s4 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 4 - theta_est)) ** 2\n",
    "    p_s2_o2 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 2 - theta_est/2)) ** 2\n",
    "    p_s4_o2 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 4 - theta_est/2)) ** 2\n",
    "\n",
    "    l_exact = np.sum(\n",
    "        np.log([1e-75 + binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk],\n",
    "                                  p_exact[kk]) for kk in\n",
    "                range(len(ula_signal.n_samples))]))\n",
    "    l_neg = np.sum(\n",
    "        np.log([1e-75 + binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk],\n",
    "                                  p_neg[kk]) for kk in\n",
    "                range(len(ula_signal.n_samples))]))\n",
    "    l_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk]*measurements[kk], ula_signal.n_samples[kk], p_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_o4 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_o4[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_same = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_same[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s4 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s4[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s2_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s2_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s4_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s4_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    \n",
    "    which_correction = np.argmax([l_same, l_s2, l_s4, l_o2, l_o4, l_s2_o2, l_s4_o2, l_neg])\n",
    "    if which_correction == 1:\n",
    "        theta_est = np.pi/2.0 - theta_est\n",
    "    elif which_correction == 2:\n",
    "        theta_est = np.pi/4.0 - theta_est\n",
    "    elif which_correction == 3:\n",
    "        theta_est = theta_est/2\n",
    "    elif which_correction == 4:\n",
    "        theta_est = theta_est/4\n",
    "    elif which_correction == 5:\n",
    "        theta_est = np.pi / 2.0 - 0.5 * theta_est\n",
    "    elif which_correction == 6:\n",
    "        theta_est = np.pi / 4.0 - 0.5 * theta_est\n",
    "    # elif which_correction == 7:\n",
    "    #     theta_est = -theta_est\n",
    "\n",
    "    # print(f'FINAL ANGLE FOUND: {theta_est, theta}')\n",
    "\n",
    "    return theta_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array: [1 1 1 1]\n",
      "\n",
      "Total variations: 8\n",
      "4*len(signs)^2: 64\n",
      "2^len(signs): 16\n",
      "\n",
      "First few variations:\n",
      "(1, -1, -1, -1)\n",
      "(1, -1, -1, 1)\n",
      "(1, -1, 1, -1)\n",
      "(1, -1, 1, 1)\n",
      "(1, 1, -1, -1)\n",
      "(1, 1, -1, 1)\n",
      "(1, 1, 1, -1)\n",
      "(1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from typing import List\n",
    "\n",
    "def generate_all_sign_variations(signs_array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate all possible sign variations by changing signs at all possible pairs of positions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signs_array : numpy.ndarray\n",
    "        Original array of signs (typically containing 1 or -1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of all possible sign variation arrays\n",
    "    \"\"\"\n",
    "    # Get all possible unique pairs of positions\n",
    "    n = len(signs_array)\n",
    "    position_pairs = list(itertools.combinations(range(n), 2))\n",
    "    \n",
    "    # Will store all variations\n",
    "    all_variations = []\n",
    "    \n",
    "    # Iterate through all possible position pairs\n",
    "    for pos1, pos2 in position_pairs:\n",
    "        # Generate variations for this pair of positions\n",
    "        pair_variations = generate_pair_variations(signs_array, pos1, pos2)\n",
    "        all_variations.extend(pair_variations)\n",
    "    \n",
    "    return all_variations\n",
    "\n",
    "def generate_adjacent_sign_variations(signs_array: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate sign variations by sliding a two-position window across the array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signs_array : numpy.ndarray\n",
    "        Original array of signs (typically containing 1 or -1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of all possible sign variation arrays\n",
    "    \"\"\"\n",
    "    # Total length of the array\n",
    "    n = len(signs_array)\n",
    "    \n",
    "    # Will store all variations\n",
    "    all_variations = []\n",
    "    \n",
    "    # Iterate through adjacent position pairs \n",
    "    # (0,1), (1,2), (2,3), ... until the second-to-last pair\n",
    "    for pos1 in range(1, n - size + 1):\n",
    "        pos = [pos1 + i for i in range(size)]\n",
    "        \n",
    "        # Generate variations for this pair of adjacent positions\n",
    "        pair_variations = generate_pair_variations(signs_array, pos)\n",
    " \n",
    "        all_variations.extend(pair_variations)\n",
    "    \n",
    "    return all_variations\n",
    "\n",
    "def generate_pair_variations(signs_array: np.ndarray, pos: List[int]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate sign variations for a specific pair of positions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signs_array : numpy.ndarray\n",
    "        Original array of signs\n",
    "    pos1 : int\n",
    "        First position to modify\n",
    "    pos2 : int\n",
    "        Second position to modify\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[numpy.ndarray]\n",
    "        List of sign variation arrays\n",
    "    \"\"\"\n",
    "    # Validate input positions\n",
    "    # if pos1 < 0 or pos2 < 0 or pos1 >= len(signs_array) or pos2 >= len(signs_array):\n",
    "    #     raise ValueError(\"Positions must be within the array bounds\")\n",
    "    \n",
    "    # Generate all possible sign combinations for the two positions\n",
    "    sign_combinations = list(itertools.product([-1, 1], repeat=len(pos)))\n",
    "    \n",
    "    # Create variations\n",
    "    variations = []\n",
    "    for combo in sign_combinations:\n",
    "        # Create a copy of the original array\n",
    "        variation = signs_array.copy()\n",
    "        \n",
    "        # Modify the two specified positions\n",
    "        for i in range(len(combo)):\n",
    "            variation[pos[i]] *= combo[i]\n",
    "        \n",
    "        variations.append(tuple(variation))\n",
    "    \n",
    "    return variations\n",
    "\n",
    "\n",
    "# Example array\n",
    "signs = np.array([1]*4)\n",
    "\n",
    "# Generate variations for all position pairs\n",
    "all_variations = generate_adjacent_sign_variations(signs, 3)\n",
    "\n",
    "print(\"Original array:\", signs)\n",
    "print(f\"\\nTotal variations: {len(all_variations)}\")\n",
    "print(f'4*len(signs)^2: {4*len(signs)**2}')\n",
    "print(f'2^len(signs): {2**len(signs)}')\n",
    "\n",
    "print(\"\\nFirst few variations:\")\n",
    "for var in all_variations:  # Print first 10 variations\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(lp, cos_signal, abs_sin, ula_signal, esprit):   \n",
    "    signal = cos_signal + 1.0j * lp * abs_sin\n",
    "    R = ula_signal.get_cov_matrix_toeplitz(signal)\n",
    "    _, _ = esprit.estimate_theta_toeplitz(R)\n",
    "    eigs = np.abs(esprit.eigs)[:2]\n",
    "    obj = eigs[1] - eigs[0]\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the distribution of the signs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "narray = [2,2,2,2,2,2,2,3]\n",
    "\n",
    "# avals = np.linspace(0.1, 0.9, 8)\n",
    "avals = [0.1*i for i in range(1, 10)]\n",
    "\n",
    "ula_signal = TwoqULASignal(M=narray, C=5)\n",
    "esprit = ESPIRIT()\n",
    "\n",
    "num_mc = 1000\n",
    "\n",
    "sign_distributions = {}\n",
    "\n",
    "for a in avals:\n",
    "    theta = np.arcsin(a)\n",
    "    distr = {}\n",
    "    for i in range(num_mc):\n",
    "        signal, _ = estimate_signal(ula_signal.depths, ula_signal.n_samples, theta)\n",
    "        # print(measurements)\n",
    "        signs = tuple(np.sign(np.imag(signal)))\n",
    "        # print(signs)\n",
    "\n",
    "        distr[signs] = distr.get(signs, 0.0) + 1/num_mc\n",
    "    sign_distributions[str(a)] = distr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 19)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**len(ula_signal.depths), len(sign_distributions['0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_signs = set()\n",
    "for a in avals:\n",
    "    distr = sign_distributions[str(a)]\n",
    "    for sign in distr.keys():\n",
    "        all_signs.add(sign)\n",
    "len(all_signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1201171875"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_signs)/2**len(ula_signal.depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "narray = [2,2,2,2,2,3]\n",
    "\n",
    "def get_heavy_signs(narray, steps):\n",
    "\n",
    "    thetas = np.linspace(np.arcsin(0.09), np.arcsin(0.91), steps)\n",
    "    avals = np.sin(thetas)\n",
    "\n",
    "    ula_signal = TwoqULASignal(M=narray, C=3)\n",
    "\n",
    "    num_mc = 1000\n",
    "\n",
    "    sign_distributions = {}\n",
    "\n",
    "    for a,theta in zip(avals,thetas):\n",
    "        # theta = np.arcsin(a)\n",
    "        distr = {}\n",
    "        for i in range(num_mc):\n",
    "            signal, _ = estimate_signal(ula_signal.depths, ula_signal.n_samples, theta)\n",
    "                # print(measurements)\n",
    "            signs = tuple(np.sign(np.imag(signal)))\n",
    "            # print(signs)\n",
    "\n",
    "            distr[signs] = distr.get(signs, 0.0) + 1/num_mc\n",
    "        sign_distributions[str(a)] = distr\n",
    "\n",
    "    def get_signs(sign_distribution):\n",
    "        # Sort the dictionary by values in descending order\n",
    "        sorted_data = dict(sorted(sign_distribution.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Create a new dictionary with cumulative sum close to 0.68\n",
    "        cumulative_dict = {}\n",
    "        cumulative_sum = 0.0\n",
    "\n",
    "        for key, value in sorted_data.items():\n",
    "            cumulative_dict[key] = value\n",
    "            if cumulative_sum + value > 0.9:\n",
    "                break\n",
    "            cumulative_sum += value\n",
    "\n",
    "        # Output the result\n",
    "        return cumulative_dict\n",
    "    \n",
    "    ret_dict = {}\n",
    "    for a in avals:\n",
    "        distr = get_signs(sign_distributions[str(a)])\n",
    "        ret_dict[str(a)] = distr\n",
    " \n",
    "    # Normalize the values\n",
    "    normalized_data = {}\n",
    "    for key, sub_dict in ret_dict.items():\n",
    "        total_sum = sum(sub_dict.values())\n",
    "        normalized_data[key] = {k: v / total_sum for k, v in sub_dict.items()}\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "normalized_data = get_heavy_signs(narray, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled keys: [(1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0)]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "# Taking a random sample based on the normalized probabilities\n",
    "def sample_from_normalized(data, sample_size=1):\n",
    "    keys = list(map(str, list(data.keys())))\n",
    "    probabilities = list(data.values())\n",
    "    sampled_keys = np.random.choice(a=keys, size=sample_size, p=probabilities)\n",
    "    return [ast.literal_eval(t) for t in sampled_keys]\n",
    "avals = list(normalized_data.keys())\n",
    "# Get one random sample\n",
    "sampled = sample_from_normalized(normalized_data[avals[0]], sample_size=3)\n",
    "print(\"Sampled keys:\",sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pooh', 'rabbit', 'rabbit', 'pooh', 'pooh'], dtype='<U11')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n",
    "np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the distribution of signs so we don't have to try all possible signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "correct signs: [ 1.  1.  1.  1. -1.  1.  1.  1.  1.]\n",
      "rough estimate a: 0.11958468443580586\n",
      "avals to use: ['0.09', '0.10663566752567158', '0.12324153604814797', '0.13981296510949756', '0.15634532387578837']\n",
      "number of signs Hamming distance two: 90\n",
      "current objective: -29.015864389995187\n",
      "current best signs: (1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0)\n",
      "current objective: -90.48784896166501\n",
      "current best signs: (1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0)\n",
      "current objective: -230.42673162944138\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0)\n",
      "current objective: -320.064581808478\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0)\n",
      "current objective: -335.92724803594695\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0)\n",
      "current objective: -349.44921488335746\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0)\n",
      "current objective: -365.5496522013772\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0)\n",
      "current objective: -365.54965220137746\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0)\n",
      "[ 1.  1.  1.  1. -1.  1.  1.  1.  1.]\n",
      "basin obj: -365.5496522013777\n",
      "true obj: -365.54965220137746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'theta_est': 0.10017347955434452,\n",
       " 'theta_est1': 0.10017347955434457,\n",
       " 'error': 6.028022874610128e-06,\n",
       " 'error1': 6.028022874651762e-06,\n",
       " 'queries': 2600,\n",
       " 'depth': 128,\n",
       " 'true_obj': -365.54965220137746,\n",
       " 'basin_obj': -365.5496522013777,\n",
       " 'x_star': array([ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.])}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.random.seed(42)\n",
    "\n",
    "a = 0.1\n",
    "print(a)\n",
    "theta = np.arcsin(a)\n",
    "\n",
    "narray = [2,2,2,2,2,2,2,2]\n",
    "heavy_signs = get_heavy_signs(narray, len(narray)**2)\n",
    "\n",
    "ula_signal = TwoqULASignal(M=narray, C=5)\n",
    "esprit = ESPIRIT()\n",
    "\n",
    "def csae_with_local_minimization(theta, ula_signal, esprit, heavy_signs, sample=False, correction=False, optimize=False, method='nelder-mead', maxiter=10, disp=False):\n",
    "\n",
    "    depths = ula_signal.depths\n",
    "    n_samples = ula_signal.n_samples\n",
    "    csignal, measurements = estimate_signal(depths, n_samples, theta)\n",
    "\n",
    "    cos_signal = np.real(csignal)\n",
    "\n",
    "    correct_signs = np.sign(np.imag(csignal))\n",
    "    if disp:\n",
    "        print(f'correct signs: {correct_signs}')\n",
    "    abs_sin = np.abs(np.imag(csignal))\n",
    "\n",
    "    obj = 1.0\n",
    "    if sample:\n",
    "        avals = list(heavy_signs.keys())\n",
    "        signs_to_try = {}\n",
    "        for a in avals:\n",
    "            signs_to_try[a] = list(set(sample_from_normalized(heavy_signs[a], sample_size=3)))\n",
    "\n",
    "        if optimize:\n",
    "            # rough estimate where the amplitude is\n",
    "            a0 = np.sqrt(0.5 - 0.5 * cos_signal[0])\n",
    "            # use rough estimate to determine which set of signs to look at\n",
    "            avals = list(map(float, avals))\n",
    "\n",
    "            # find first element in avals that is larger than a0\n",
    "            left = 0\n",
    "            right = len(avals)\n",
    "            \n",
    "            while left < right:\n",
    "                mid = (left + right) // 2\n",
    "                \n",
    "                if avals[mid] <= a0:\n",
    "                    left = mid + 1\n",
    "                else:\n",
    "                    right = mid\n",
    "            L=3\n",
    "            if left >= 4:\n",
    "                avals_to_use = list(map(str, avals[left-L: left+L]))\n",
    "            if left >= 3:\n",
    "                avals_to_use = list(map(str, avals[left-3: left+L]))\n",
    "            elif left>=2:\n",
    "                avals_to_use = list(map(str, avals[left-2: left+L]))\n",
    "            elif left>=1:\n",
    "                avals_to_use = list(map(str, avals[left-1: left+L]))\n",
    "            else:\n",
    "                avals_to_use = list(map(str, avals[left: left+L]))\n",
    "\n",
    "            if disp:\n",
    "                print(f'rough estimate a: {a0}')\n",
    "                print(f'avals to use: {avals_to_use}')\n",
    "\n",
    "            all_signs = []\n",
    "            for a in avals_to_use:\n",
    "                for x in signs_to_try[a]:\n",
    "                    hamming_distance_two_signs = generate_adjacent_sign_variations(np.array(x), 2)\n",
    "                    all_signs.extend(hamming_distance_two_signs)\n",
    "            all_signs = list(set(all_signs))\n",
    "            if disp:\n",
    "                print(f'number of signs Hamming distance two: {len(all_signs)}')\n",
    "\n",
    "            for x in all_signs:\n",
    "                curr_obj = objective_function(np.array(x), cos_signal, abs_sin, ula_signal, esprit)\n",
    "                if curr_obj<obj:\n",
    "                    if disp:\n",
    "                        print(f'current objective: {curr_obj}')\n",
    "                        print(f'current best signs: {x}')\n",
    "                    obj = curr_obj\n",
    "                    x_star = np.array(x)\n",
    "\n",
    "            # do one more sweep\n",
    "\n",
    "            hamming_distance_one_signs = list(set(generate_adjacent_sign_variations(x_star, 1)))\n",
    "            # print(f'number of signs Hamming distance one: {len(hamming_distance_one_signs)}')\n",
    "            for x in hamming_distance_one_signs:\n",
    "                curr_obj = objective_function(np.array(x), cos_signal, abs_sin, ula_signal, esprit)\n",
    "                if curr_obj<obj:\n",
    "                    if disp:\n",
    "                        print(f'current objective: {curr_obj}')\n",
    "                        print(f'current best signs: {x}')\n",
    "                    obj = curr_obj\n",
    "                    x_star = np.array(x)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        else:\n",
    "            all_signs = []\n",
    "            for a in avals:\n",
    "                all_signs.extend(heavy_signs[a])\n",
    "            all_signs = list(set(all_signs))\n",
    "            for x in all_signs:\n",
    "                basin_obj = objective_function(np.array(x), cos_signal, abs_sin, ula_signal, esprit)\n",
    "                if basin_obj < obj:\n",
    "                    obj = basin_obj\n",
    "                    x_star = np.array(x)\n",
    "    else:\n",
    "        avals = list(heavy_signs.keys())\n",
    "        signs_to_try = {}\n",
    "        for a in avals:\n",
    "            signs_to_try[a] = heavy_signs[a]\n",
    "\n",
    "        if optimize:\n",
    "            # rough estimate where the amplitude is\n",
    "            a0 = np.sqrt(0.5 - 0.5 * cos_signal[0])\n",
    "            # use rough estimate to determine which set of signs to look at\n",
    "            avals = list(map(float, avals))\n",
    "\n",
    "            # find first element in avals that is larger than a0\n",
    "            left = 0\n",
    "            right = len(avals)\n",
    "            \n",
    "            while left < right:\n",
    "                mid = (left + right) // 2\n",
    "                \n",
    "                if avals[mid] <= a0:\n",
    "                    left = mid + 1\n",
    "                else:\n",
    "                    right = mid\n",
    "            L=2\n",
    "            if left >= 4:\n",
    "                avals_to_use = list(map(str, avals[left-L: left+L]))\n",
    "            if left >= 3:\n",
    "                avals_to_use = list(map(str, avals[left-3: left+L]))\n",
    "            elif left>=2:\n",
    "                avals_to_use = list(map(str, avals[left-2: left+L]))\n",
    "            elif left>=1:\n",
    "                avals_to_use = list(map(str, avals[left-1: left+L]))\n",
    "            else:\n",
    "                avals_to_use = list(map(str, avals[left: left+L]))\n",
    "\n",
    "            if disp:\n",
    "                print(f'rough estimate a: {a0}')\n",
    "                print(f'avals to use: {avals_to_use}')\n",
    "\n",
    "            all_signs = []\n",
    "            for a in avals_to_use:\n",
    "                for x in signs_to_try[a]:\n",
    "                    hamming_distance_two_signs = generate_adjacent_sign_variations(np.array(x))\n",
    "                    all_signs.extend(hamming_distance_two_signs)\n",
    "            all_signs = list(set(all_signs))\n",
    "            if disp:\n",
    "                print(f'number of signs Hamming distance two: {len(all_signs)}')\n",
    "\n",
    "            for x in all_signs:\n",
    "                curr_obj = objective_function(np.array(x), cos_signal, abs_sin, ula_signal, esprit)\n",
    "                if curr_obj<obj:\n",
    "                    if disp:\n",
    "                        print(f'current objective: {curr_obj}')\n",
    "                        print(f'current best signs: {x}')\n",
    "                    obj = curr_obj\n",
    "                    x_star = np.array(x)\n",
    "\n",
    "\n",
    "        else:\n",
    "            all_signs = []\n",
    "            for a in avals:\n",
    "                all_signs.extend(heavy_signs[a])\n",
    "            all_signs = list(set(all_signs))\n",
    "            if disp:\n",
    "                print(f'number of signs to try: {len(all_signs)}')\n",
    "            for x in all_signs:\n",
    "                basin_obj = objective_function(np.array(x), cos_signal, abs_sin, ula_signal, esprit)\n",
    "                if basin_obj < obj:\n",
    "                    obj = basin_obj\n",
    "                    x_star = np.array(x)\n",
    "    if disp:\n",
    "        print(x_star)\n",
    "        \n",
    "    signal = cos_signal + 1.0j * x_star * abs_sin\n",
    "    R = ula_signal.get_cov_matrix_toeplitz(signal)\n",
    "    theta_est, _ = esprit.estimate_theta_toeplitz(R)\n",
    "    eigs = np.abs(esprit.eigs)[:2]\n",
    "    basin_obj = eigs[1] - eigs[0]\n",
    "    if correction:\n",
    "        theta_est = apply_correction(ula_signal, measurements, theta_est, theta) #apply correction\n",
    "    if disp:\n",
    "        print(f'basin obj: {basin_obj}')\n",
    "\n",
    "    cR = ula_signal.get_cov_matrix_toeplitz(csignal)\n",
    "    theta_est1, _ = esprit.estimate_theta_toeplitz(cR)\n",
    "    eigs = np.abs(esprit.eigs)[:2]\n",
    "    true_obj = eigs[1] - eigs[0]\n",
    "    if disp:\n",
    "        print(f'true obj: {true_obj}')\n",
    "\n",
    "    num_queries = np.sum(np.array(ula_signal.depths)*np.array(ula_signal.n_samples)) + ula_signal.n_samples[0]\n",
    "    # Compute the maximum single query\n",
    "    max_single_query = np.max(ula_signal.depths)\n",
    "\n",
    "    ret_dict = {'theta_est': theta_est, 'theta_est1': theta_est1, \n",
    "                'error': np.abs(np.sin(theta)-np.sin(theta_est)), \n",
    "                'error1': np.abs(np.sin(theta)-np.sin(theta_est1)), \n",
    "                'queries': num_queries, 'depth': max_single_query,\n",
    "                'true_obj': true_obj, 'basin_obj': basin_obj,\n",
    "                'x_star': x_star}\n",
    "    \n",
    "    return ret_dict\n",
    "\n",
    "csae_with_local_minimization(theta, ula_signal, esprit, heavy_signs, sample=True, correction=True, optimize=True, disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some statistics now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta = 0.1001674211615598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 5.153695491652375, 0.41024441724596017\n",
      "theta = 0.2013579207903308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 254.1935620168903, 20.234313394379328\n",
      "theta = 0.30469265401539747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 225.0082163329581, 17.911101797648406\n",
      "theta = 0.41151684606748806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 35.10395713055236, 2.7943448462131233\n",
      "theta = 0.5235987755982988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 3.300805248660273, 0.2627506665600217\n",
      "theta = 0.6435011087932844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.4893902026405312, 0.1981604141405398\n",
      "theta = 0.775397496610753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 120.56505786781861, 9.59721853674178\n",
      "theta = 0.9272952180016123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 1.8277744461952063, 0.1454944832792204\n",
      "theta = 1.1197695149986342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 99.78142913657155, 7.9428003292793266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "avals = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# avals = [0.2]\n",
    "# avals = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "narray = [2]*6\n",
    "heavy_signs = get_heavy_signs(narray, len(narray))\n",
    "num_queries = np.zeros(len(avals), dtype=int)\n",
    "max_single_query = np.zeros(len(avals), dtype=int)\n",
    "\n",
    "num_mc=100\n",
    "thetas = np.zeros((len(avals), num_mc))\n",
    "errors = np.zeros((len(avals), num_mc))\n",
    "thetas1 = np.zeros((len(avals), num_mc))\n",
    "errors1 = np.zeros((len(avals), num_mc))\n",
    "\n",
    "basin_obj = np.zeros((len(avals), num_mc))\n",
    "true_obj = np.zeros((len(avals), num_mc))\n",
    "\n",
    "ula_signal = TwoqULASignal(M=narray, C=3)\n",
    "esprit = ESPIRIT()\n",
    "\n",
    "for j,a in enumerate(avals):\n",
    "    theta = np.arcsin(a)\n",
    "    print(f'theta = {theta}')\n",
    "    for i in tqdm(range(num_mc)):\n",
    "        res = csae_with_local_minimization(theta, ula_signal, esprit, heavy_signs, sample=False, optimize=False, disp=False)\n",
    "        thetas[j][i] = res['theta_est']\n",
    "        errors[j][i] = res['error']\n",
    "\n",
    "        thetas1[j][i] = res['theta_est1']\n",
    "        errors1[j][i] = res['error1']\n",
    "\n",
    "        basin_obj[j][i] = res['basin_obj']\n",
    "        true_obj[j][i] = res['true_obj']\n",
    "    num_queries[j] = res['queries']\n",
    "    max_single_query[j] = res['depth']\n",
    "\n",
    "\n",
    "    print(f'constant factors query and depth: {np.percentile(errors[j], 95) * num_queries[j]}, {np.percentile(errors[j], 95) * max_single_query[j]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69\n",
      "0.18\n",
      "1.0\n",
      "0.88\n",
      "0.8\n",
      "1.0\n",
      "0.45\n",
      "1.0\n",
      "0.13\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(avals)):\n",
    "    print(np.sum(np.where(np.array(basin_obj[i])/np.array(true_obj[i]) >= 0.99, 1, 0))/len(basin_obj[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning the distribution of signs...\n",
      "theta = 0.1001674211615598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 7.985125513803864, 0.381379129017498\n",
      "theta = 0.2013579207903308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 419.77067521297334, 20.04874866688828\n",
      "theta = 0.30469265401539747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 391.1371478668768, 18.681177211552324\n",
      "theta = 0.41151684606748806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 3.7460819341544327, 0.17891734610886842\n",
      "theta = 0.5235987755982988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 4.271665498072618, 0.20401984468406537\n",
      "theta = 0.6435011087932844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 4.32281401797002, 0.20646275906722486\n",
      "theta = 0.775397496610753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 464.716762893706, 22.195427481490437\n",
      "theta = 0.9272952180016123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.543427513974494, 0.12147713499579672\n",
      "theta = 1.1197695149986342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 1.7539317381755122, 0.08376987406211402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "\n",
    "avals = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# avals = [np.random.uniform(0.1, 0.9)]\n",
    "# avals = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# avals = [0.1, 0.2, 0.3]\n",
    "# avals = [np.random.uniform(0.1, 0.4) for _ in range(3)]\n",
    "# narray = [2]*5 + [3]\n",
    "narray = [2]*6\n",
    "\n",
    "print('learning the distribution of signs...')\n",
    "heavy_signs = get_heavy_signs(narray, int(4*len(narray)))\n",
    "num_queries = np.zeros(len(avals), dtype=int)\n",
    "max_single_query = np.zeros(len(avals), dtype=int)\n",
    "\n",
    "num_mc=100\n",
    "thetas = np.zeros((len(avals), num_mc))\n",
    "errors = np.zeros((len(avals), num_mc))\n",
    "thetas1 = np.zeros((len(avals), num_mc))\n",
    "errors1 = np.zeros((len(avals), num_mc))\n",
    "\n",
    "basin_obj = np.zeros((len(avals), num_mc))\n",
    "true_obj = np.zeros((len(avals), num_mc))\n",
    "\n",
    "ula_signal = TwoqULASignal(M=narray, C=5)\n",
    "esprit = ESPIRIT()\n",
    "\n",
    "for j,a in enumerate(avals):\n",
    "    theta = np.arcsin(a)\n",
    "    print(f'theta = {theta}')\n",
    "    for i in tqdm(range(num_mc)):\n",
    "        res = csae_with_local_minimization(theta, ula_signal, esprit, heavy_signs, sample=True, correction=True, optimize=True, disp=False)\n",
    "        thetas[j][i] = res['theta_est']\n",
    "        errors[j][i] = res['error']\n",
    "\n",
    "        thetas1[j][i] = res['theta_est1']\n",
    "        errors1[j][i] = res['error1']\n",
    "\n",
    "        basin_obj[j][i] = res['basin_obj']\n",
    "        true_obj[j][i] = res['true_obj']\n",
    "    num_queries[j] = res['queries']\n",
    "    max_single_query[j] = res['depth']\n",
    "\n",
    "    print(f'constant factors query and depth: {np.percentile(errors[j], 95) * num_queries[j]}, {np.percentile(errors[j], 95) * max_single_query[j]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.95\n",
      "1.0\n",
      "1.0\n",
      "0.98\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(avals)):\n",
    "    print(np.sum(np.where(np.array(basin_obj[i])/np.array(true_obj[i]) >= 0.99, 1, 0))/len(basin_obj[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.        , 1.        , 1.        , 1.        , 0.79742077,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.89037927, 1.        ,\n",
       "        1.21145694, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.2859767 , 1.        , 1.12457518, 1.        , 1.        ,\n",
       "        1.78676735, 1.        , 1.09322653, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.14565714, 1.        , 1.        ,\n",
       "        1.28146141, 1.        , 1.        , 1.42428746, 1.        ,\n",
       "        0.93172308, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.01643466, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.50064494, 1.46471884, 0.8309843 , 1.        , 1.03773762,\n",
       "        1.15892483, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.92098881, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.09960314, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.48722322, 1.        , 1.        , 1.01084954, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.06132198, 1.        , 1.        , 1.11223893, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 0.95)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=1\n",
    "np.array(basin_obj[i])/np.array(true_obj[i]), np.sum(np.where(np.array(basin_obj[i])/np.array(true_obj[i]) >= 0.99, 1, 0))/len(basin_obj[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.19864555,  0.19732776,  0.20147648,  0.19935255,  0.97901182,\n",
       "         0.19837722,  0.97959974,  0.20620379,  0.1960183 ,  0.19735133,\n",
       "         0.19735874,  0.19940808,  0.19592142,  0.18426184,  0.19852466,\n",
       "         0.18100445,  0.20010377,  0.19901248,  0.19966421,  0.20247303,\n",
       "         0.18334523,  0.20159819,  0.17836658,  0.20103027,  0.20171743,\n",
       "         0.20239274,  0.20004671,  0.17625892,  0.19996035,  0.19826194,\n",
       "         0.19913511,  0.19986371, -0.20113753,  0.20082354,  0.19845501,\n",
       "         0.82314453,  0.19837813,  0.19696225,  0.17736816,  0.19974173,\n",
       "         0.18584096,  0.19894422,  0.19901455,  0.19840489,  0.20352749,\n",
       "         0.97908573,  0.20588665,  0.18668135,  0.19698039,  0.2003974 ,\n",
       "         0.19935703,  0.20025191,  0.20225178,  0.20000534,  0.20090939,\n",
       "         0.1808781 ,  0.17870773,  0.17881389,  0.20133488,  0.1781964 ,\n",
       "         0.19779547,  0.19849779,  0.1974146 ,  0.2000387 ,  0.19950217,\n",
       "         0.20031544,  0.97912415,  0.19833217,  0.19991416,  0.20008624,\n",
       "         0.19750155,  0.20286509,  0.20034745,  0.20114662,  0.19829976,\n",
       "         0.18575611,  0.19900443,  0.19934743,  0.19768019,  0.19737403,\n",
       "         0.82607298,  0.19996047,  0.19990064,  0.18256767,  0.20133211,\n",
       "         0.20047648,  0.20000066,  0.20301383,  0.20267776,  0.20373943,\n",
       "         0.18075779,  0.20029   ,  0.20061253,  0.83508121,  0.19874646,\n",
       "         0.19551907,  0.19658923,  0.20133422,  0.2015103 ,  0.20303947]),\n",
       " 0.2)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sin(thetas[i]), avals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth (68%) for 0.1: 2.472655951649311, 0.11809700067578799\n",
      "constant factors query and depth (68%) for 0.2: 2.0356584517953027, 0.09722547829470102\n",
      "constant factors query and depth (68%) for 0.3: 2.141208816541388, 0.10226668974526032\n",
      "constant factors query and depth (68%) for 0.4: 2.357667215430917, 0.11260500133401394\n",
      "constant factors query and depth (68%) for 0.5: 1.8149934132994892, 0.08668625257549799\n",
      "constant factors query and depth (68%) for 0.6: 1.5734870043493923, 0.07515161811817993\n",
      "constant factors query and depth (68%) for 0.7: 4.142049166458483, 0.19782921392040514\n",
      "constant factors query and depth (68%) for 0.8: 1.249651043171467, 0.059684825942517825\n",
      "constant factors query and depth (68%) for 0.9: 0.7117324333370024, 0.03399319084594638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.055455944003639"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc = 68\n",
    "avg = 0.0\n",
    "for i in range(len(avals)):\n",
    "    avg += np.percentile(errors[i], perc) * num_queries[i]/len(avals)\n",
    "    print(f'constant factors query and depth ({perc}%) for {avals[i]}: {np.percentile(errors[i], perc) * num_queries[i]}, {np.percentile(errors[i], perc) * max_single_query[i]}')\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth with known signs for 0.9: 4.547317164466012, 0.21718529740733192\n",
      "constant factors query and depth with known signs for 0.9: 3.1275321650936894, 0.1493746705716389\n",
      "constant factors query and depth with known signs for 0.9: 3.583304266584761, 0.1711428903443468\n",
      "constant factors query and depth with known signs for 0.9: 4.289963583207526, 0.2048937830785684\n",
      "constant factors query and depth with known signs for 0.9: 3.7532407795766995, 0.17925926111411103\n",
      "constant factors query and depth with known signs for 0.9: 2.888015815863788, 0.13793508374274807\n",
      "constant factors query and depth with known signs for 0.9: 3.256112205514157, 0.1555158068305269\n",
      "constant factors query and depth with known signs for 0.9: 2.4866124464633956, 0.1187635795325801\n",
      "constant factors query and depth with known signs for 0.9: 1.3564737840641954, 0.0647868075970959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.2542858012038027"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc = 68\n",
    "avg = 0.0\n",
    "for j in range(len(avals)):\n",
    "    avg += 2*np.percentile(errors1[j], perc) * num_queries[j]/len(avals)\n",
    "    print(f'constant factors query and depth with known signs for {avals[i]}: {2*np.percentile(errors1[j], perc) * num_queries[j]}, {2*np.percentile(errors1[j], perc) * max_single_query[j]}')\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 57, 106, 208, 330, 461]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q = 7\n",
    "# lengths = []\n",
    "# for q in range(2, Q + 1):\n",
    "#     narray = [2]*(2*q)\n",
    "#     heavy_signs = get_heavy_signs(narray, 4*len(narray))\n",
    "#     signs_to_try = []\n",
    "#     for a in heavy_signs.keys():\n",
    "#         signs = heavy_signs[a].keys()\n",
    "#         signs_to_try += signs\n",
    "#     signs_to_try = list(set(signs_to_try))\n",
    "#     lengths.append(len(signs_to_try))\n",
    "\n",
    "# lengths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
