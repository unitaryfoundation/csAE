{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from signals import *\n",
    "from frequencyestimator import *\n",
    "from scipy.optimize import basinhopping\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "sns.set_context(\"poster\", font_scale = .45, rc={\"grid.linewidth\": 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = lambda n, theta: np.cos((2*n+1)*theta)**2\n",
    "P1 = lambda n, theta: np.sin((2*n+1)*theta)**2\n",
    "\n",
    "def estimate_signal(depths, n_samples, theta, eta=0.0):\n",
    "        signals = np.zeros(len(depths), dtype = np.complex128)\n",
    "        measurements = np.zeros(len(depths))\n",
    "        cos_signal = np.zeros(len(depths), dtype = np.complex128)\n",
    "        for i,n in enumerate(depths):\n",
    "            # Get the exact measuremnt probabilities\n",
    "            p0 = P0(n, theta)\n",
    "            p1 = P1(n, theta)\n",
    "\n",
    "            p0x = P0x(n,theta)\n",
    "            p1x = P1x(n,theta)\n",
    "\n",
    "            # Get the \"noisy\" probabilities by sampling and adding a bias term that pushes towards 50/50 mixture\n",
    "            eta_n = (1.0-eta)**(n+1) # The error at depth n increases as more queries are implemented\n",
    "            p0_estimate = np.random.binomial(n_samples[i], eta_n*p0 + (1.0-eta_n)*0.5)/n_samples[i]\n",
    "            p1_estimate = 1 - p0_estimate\n",
    "\n",
    "            p0x_estimate = np.random.binomial(n_samples[i], eta_n*p0x + (1.0-eta_n)*0.5)/n_samples[i]\n",
    "            p1x_estimate = 1.0 - p0x_estimate\n",
    "            measurements[i] = p0_estimate\n",
    "            \n",
    "            # Estimate theta\n",
    "            theta_estimated = np.arctan2(p0x_estimate - p1x_estimate, p0_estimate - p1_estimate)\n",
    "            \n",
    "            # Store this to determine angle at theta = 0 or pi/2\n",
    "            if i==0:\n",
    "                p0mp1 = p0_estimate - p1_estimate\n",
    "\n",
    "            # Compute f(n) - Eq. 3\n",
    "            fi_estimate = np.exp(1.0j*theta_estimated)\n",
    "            signals[i] = fi_estimate\n",
    "         \n",
    "        return signals, measurements\n",
    "\n",
    "from scipy.stats import binom\n",
    "\n",
    "def apply_correction(ula_signal, measurements, theta_est, theta):\n",
    "    p_exact = np.cos((2*ula_signal.depths+1)*(theta))**2\n",
    "    p_o2 = np.cos((2 * ula_signal.depths + 1) * (theta_est/2.0)) ** 2\n",
    "    p_o4 = np.cos((2 * ula_signal.depths + 1) * (theta_est/4.0)) ** 2\n",
    "    p_same = np.cos((2*ula_signal.depths+1)*(theta_est))**2\n",
    "    p_s2 = np.cos((2 * ula_signal.depths + 1) * (np.pi/2-theta_est)) ** 2\n",
    "    p_s4 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 4 - theta_est)) ** 2\n",
    "    p_s2_o2 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 2 - theta_est/2)) ** 2\n",
    "    p_s4_o2 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 4 - theta_est/2)) ** 2\n",
    "\n",
    "    l_exact = np.sum(\n",
    "        np.log([1e-75 + binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk],\n",
    "                                  p_exact[kk]) for kk in\n",
    "                range(len(ula_signal.n_samples))]))\n",
    "\n",
    "    l_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk]*measurements[kk], ula_signal.n_samples[kk], p_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_o4 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_o4[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_same = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_same[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s4 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s4[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s2_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s2_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s4_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s4_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    # print(f'theta_exact obj:           {l_exact}\\n')\n",
    "\n",
    "    # print(f'2*theta_found obj:         {l_same}')\n",
    "    # print(f'2*theta found:             {2*theta_est / np.pi}\\n')\n",
    "\n",
    "    # print(f'pi-2*theta_found obj:      {l_s2}')\n",
    "    # print(f'pi-2*theta found:          {1.0-2 * theta_est / np.pi}\\n')\n",
    "\n",
    "    # print(f'pi/2-2*theta_found obj:    {l_s4}')\n",
    "    # print(f'pi/2-2*theta found:        {0.5 - 2 * theta_est / np.pi}\\n')\n",
    "\n",
    "    # print(f'theta_found obj:           {l_o2}')\n",
    "    # print(f'theta found:               {theta_est / np.pi}\\n')\n",
    "\n",
    "    # print(f'theta_found/2 obj:         {l_o4}')\n",
    "    # print(f'theta found:               {theta_est / 2.0 / np.pi}\\n')\n",
    "\n",
    "    # print(f'pi - theta_found obj:    {l_s2_o2}')\n",
    "    # print(f'pi - theta found:        {1.0 - theta_est / np.pi}\\n')\n",
    "\n",
    "    # print(f'pi/2 - theta_found obj:    {l_s4_o2}')\n",
    "    # print(f'pi/2 - theta found:        {0.5 - theta_est / np.pi}\\n')\n",
    "\n",
    "    \n",
    "    which_correction = np.argmax([l_same, l_s2, l_s4, l_o2, l_o4, l_s2_o2, l_s4_o2])\n",
    "    if which_correction == 1:\n",
    "        theta_est = np.pi/2.0 - theta_est\n",
    "    elif which_correction == 2:\n",
    "        theta_est = np.pi/4.0 - theta_est\n",
    "    elif which_correction == 3:\n",
    "        theta_est = theta_est/2\n",
    "    elif which_correction == 4:\n",
    "        theta_est = theta_est/4\n",
    "    elif which_correction == 5:\n",
    "        theta_est = np.pi / 2.0 - 0.5 * theta_est\n",
    "    elif which_correction == 6:\n",
    "        theta_est = np.pi / 4.0 - 0.5 * theta_est\n",
    "\n",
    "    # print(f'FINAL ANGLE FOUND: {theta_est, theta}')\n",
    "\n",
    "    return theta_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Basinhopping (simulated annealing) from scipy.optimize to minimize the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(lp, cos_signal, abs_sin, ula_signal, esprit):   \n",
    "    signal = cos_signal + 1.0j * lp * abs_sin\n",
    "    R = ula_signal.get_cov_matrix_toeplitz(signal)\n",
    "    _, _ = esprit.estimate_theta_toeplitz(R)\n",
    "    eigs = np.abs(esprit.eigs)[:2]\n",
    "    obj = eigs[1] - eigs[0]\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to implement the basinhopping based csAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basinhopping step 0: f -385.792\n",
      "basinhopping step 1: f -385.792 trial_f -378.567 accepted 0  lowest_f -385.792\n",
      "basinhopping step 2: f -385.792 trial_f -69.3571 accepted 0  lowest_f -385.792\n",
      "basinhopping step 3: f -385.792 trial_f -65.5468 accepted 0  lowest_f -385.792\n",
      "basinhopping step 4: f -385.792 trial_f -385.792 accepted 1  lowest_f -385.792\n",
      "found new global minimum on step 4 with function value -385.792\n",
      "basinhopping step 5: f -385.792 trial_f -60.1884 accepted 0  lowest_f -385.792\n",
      "basinhopping step 6: f -385.792 trial_f -69.3623 accepted 0  lowest_f -385.792\n",
      "basinhopping step 7: f -385.792 trial_f -385.792 accepted 1  lowest_f -385.792\n",
      "found new global minimum on step 7 with function value -385.792\n",
      "basinhopping step 8: f -385.792 trial_f -87.0543 accepted 0  lowest_f -385.792\n",
      "basinhopping step 9: f -385.792 trial_f -100.399 accepted 0  lowest_f -385.792\n",
      "basinhopping step 10: f -385.792 trial_f -385.792 accepted 1  lowest_f -385.792\n",
      "basinhopping step 11: f -385.792 trial_f -66.3175 accepted 0  lowest_f -385.792\n",
      "basinhopping step 12: f -385.792 trial_f -385.792 accepted 1  lowest_f -385.792\n",
      "found new global minimum on step 12 with function value -385.792\n",
      "basinhopping step 13: f -385.792 trial_f -65.5468 accepted 0  lowest_f -385.792\n",
      "basinhopping step 14: f -385.792 trial_f -43.6132 accepted 0  lowest_f -385.792\n",
      "basinhopping step 15: f -385.792 trial_f -32.353 accepted 0  lowest_f -385.792\n",
      "basinhopping step 16: f -385.792 trial_f -32.353 accepted 0  lowest_f -385.792\n",
      "basinhopping step 17: f -385.792 trial_f -385.792 accepted 1  lowest_f -385.792\n",
      "basinhopping step 18: f -385.792 trial_f -40.2411 accepted 0  lowest_f -385.792\n",
      "basinhopping step 19: f -385.792 trial_f -385.792 accepted 1  lowest_f -385.792\n",
      "basinhopping step 20: f -385.792 trial_f -43.6132 accepted 0  lowest_f -385.792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'theta_est': 0.7755034649040587,\n",
       " 'error': 7.567256784446474e-05,\n",
       " 'queries': 5065,\n",
       " 'depth': 128}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0.7\n",
    "theta = np.arcsin(a)\n",
    "\n",
    "narray = [2,2,2,2,2,2,2,2]\n",
    "# narray = [2]*8\n",
    "\n",
    "def csae_with_basinhopping(theta, narray, niter, T=1.0, disp=False):\n",
    "\n",
    "    ula_signal = TwoqULASignal(M=narray, C=5)\n",
    "    esprit = ESPIRIT()\n",
    "\n",
    "    depths = ula_signal.depths\n",
    "    n_samples = ula_signal.n_samples\n",
    "    csignal, measurements = estimate_signal(depths, n_samples, theta)\n",
    "\n",
    "    cos_signal = np.real(csignal)\n",
    "\n",
    "    correct_signs = np.sign(np.imag(csignal))\n",
    "    abs_sin = np.abs(np.imag(csignal))\n",
    "\n",
    "    bounds = tuple((-1.0, 1.0) for _ in range(len(correct_signs)))\n",
    "    init_signs = np.array([1.0 for _ in range(len(correct_signs))])\n",
    "    res = basinhopping(objective_function, init_signs, niter=niter, T=1.0, stepsize=2.0, minimizer_kwargs={'args': (cos_signal, abs_sin, ula_signal, esprit), 'bounds': bounds}, disp=disp)\n",
    "\n",
    "    x = res.x\n",
    "    signal = cos_signal + 1.0j * x * abs_sin\n",
    "    R = ula_signal.get_cov_matrix_toeplitz(signal)\n",
    "    theta_est, _ = esprit.estimate_theta_toeplitz(R)\n",
    "    theta_est = apply_correction(ula_signal, measurements, theta_est, theta) #apply correction\n",
    "\n",
    "    # cR = ula_signal.get_cov_matrix_toeplitz(csignal)\n",
    "    # theta_est, _ = esprit.estimate_theta_toeplitz(cR)\n",
    "    # eigs = np.abs(esprit.eigs)[:2]\n",
    "    # obj = eigs[1] - eigs[0]\n",
    "    # print(f'true obj: {obj}')\n",
    "\n",
    "    num_queries = 2*np.sum(np.array(ula_signal.depths)*np.array(ula_signal.n_samples)) + ula_signal.n_samples[0]\n",
    "    # Compute the maximum single query\n",
    "    max_single_query = np.max(ula_signal.depths)\n",
    "\n",
    "    ret_dict = {'theta_est': theta_est, 'error': np.abs(np.sin(theta)-np.sin(theta_est)), 'queries': num_queries, 'depth': max_single_query}\n",
    "    \n",
    "    return ret_dict\n",
    "\n",
    "csae_with_basinhopping(theta=theta, narray=narray, niter=20, disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some statistics now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta = 0.1001674211615598\n",
      "theta = 0.2013579207903308\n",
      "theta = 0.30469265401539747\n",
      "theta = 0.41151684606748806\n",
      "theta = 0.5235987755982988\n",
      "theta = 0.6435011087932844\n",
      "theta = 0.775397496610753\n",
      "theta = 0.9272952180016123\n",
      "theta = 1.1197695149986342\n"
     ]
    }
   ],
   "source": [
    "avals = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "narray = [2,2,2,2,2,3]\n",
    "\n",
    "num_queries = np.zeros(len(avals), dtype=int)\n",
    "max_single_query = np.zeros(len(avals), dtype=int)\n",
    "\n",
    "num_mc=100\n",
    "thetas = np.zeros((len(avals), num_mc))\n",
    "errors = np.zeros((len(avals), num_mc))\n",
    "\n",
    "for j,a in enumerate(avals):\n",
    "    theta = np.arcsin(a)\n",
    "    print(f'theta = {theta}')\n",
    "    for i in range(num_mc):\n",
    "        res = csae_with_basinhopping(theta=theta, narray=narray, niter=20)\n",
    "        thetas[j][i] = res['theta_est']\n",
    "        errors[j][i] = res['error']\n",
    "    num_queries[j] = res['queries']\n",
    "    max_single_query[j] = res['depth']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors for query complexity: [18.79593714  9.89123571  6.6135364  12.74532996  5.20001127  9.44640659\n",
      " 12.70436438  6.83696331 65.89290219]\n",
      "constant factors for max depth complexity: [0.47925896 0.25220681 0.168632   0.32498052 0.13258993 0.24086455\n",
      " 0.32393598 0.17432894 1.68013775]\n"
     ]
    }
   ],
   "source": [
    "errors95 = np.percentile(errors, 95, axis=1)\n",
    "print(f'constant factors for query complexity: {errors95*num_queries}'),\n",
    "print(f'constant factors for max depth complexity: {errors95*max_single_query}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
