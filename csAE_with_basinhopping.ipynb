{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from signals import *\n",
    "from frequencyestimator import *\n",
    "from scipy.optimize import basinhopping, minimize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "sns.set_context(\"poster\", font_scale = .45, rc={\"grid.linewidth\": 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import pickle\n",
    "from scipy.linalg import toeplitz\n",
    "from numba import njit\n",
    "\n",
    "P0 = lambda n, theta: np.cos((2*n+1)*theta)**2\n",
    "P1 = lambda n, theta: np.sin((2*n+1)*theta)**2\n",
    "P0x = lambda n, theta: (1.0 + np.sin(2*(2*n+1)*theta))/2.0\n",
    "P1x = lambda n, theta: (1.0 - np.sin(2*(2*n+1)*theta))/2.0\n",
    "\n",
    "@njit\n",
    "def get_ula_signal(q, idx, signal):\n",
    "    p = np.outer(signal, np.conj(signal)).T.ravel()  # Compute outer product\n",
    "    p = p[idx[0]]  # Restrict to indices\n",
    "    cp = np.conj(p)\n",
    "    for i in range(1, q):\n",
    "        p = np.outer(p, cp).T.ravel() # Compute outer product iteratively\n",
    "        p = p[idx[i]]  # Restrict to indices\n",
    "    return p\n",
    "\n",
    "class ULASignal(metaclass = ABCMeta):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_cov_matrix(self):\n",
    "        pass\n",
    "    \n",
    "class TwoqULASignal(ULASignal):\n",
    "\n",
    "    def __init__(self, M=None, ula=None, seed=None, C=1.2):\n",
    "        '''\n",
    "        Constructor for wrapper class around signal.\n",
    "            ULA_signal_dict: either a dictionary containing the signal or path to a pickle file to load with the signal\n",
    "        '''\n",
    "        if seed: np.random.seed(seed)\n",
    "        \n",
    "        if isinstance(M, (list, np.ndarray)):\n",
    "            self.M = M\n",
    "            depths, n_samples = self._get_depths(self.M, C=C)\n",
    "            self.depths = depths\n",
    "            self.n_samples = n_samples\n",
    "            self.q = len(self.M)//2 if len(self.M) % 2 == 0 else len(self.M)//2 + 1\n",
    "            self.idx = self.get_idx()\n",
    "        elif isinstance(ula, str):\n",
    "            with open(ula, 'rb') as handle:\n",
    "                self.idx, self.depths, self.n_samples, self.M = pickle.load(handle)\n",
    "            self.q = len(self.M)//2 if len(self.M) % 2 == 0 else len(self.M)//2 + 1\n",
    "        else:\n",
    "            raise TypeError(\"Input ULA must by array of indices or path to pickle file\")\n",
    "\n",
    "    def save_ula(self, filename='ula.pkl'):\n",
    "        with open(filename, 'wb') as handle:\n",
    "            pickle.dump((self.idx, self.depths, self.n_samples, self.M), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    \n",
    "    def get_cov_matrix(self, signal):\n",
    "        '''\n",
    "        This generates Eq. 13 in the paper DOI: 10.1109/DSP-SPE.2011.5739227 using the \n",
    "        technique from DOI:10.1109/LSP.2015.2409153\n",
    "        '''\n",
    "        self.ULA_signal = self.get_ula_signal(self.q, self.idx, signal)\n",
    "        total_size = len(self.ULA_signal)\n",
    "        ULA_signal = self.ULA_signal\n",
    "\n",
    "        '''\n",
    "        This uses the techinque from DOI:10.1109/LSP.2015.2409153\n",
    "        '''\n",
    "        subarray_col = ULA_signal[total_size//2:]\n",
    "        subarray_row = np.conj(subarray_col)\n",
    "        covariance_matrix = toeplitz(subarray_col, subarray_row)\n",
    "        \n",
    "        \n",
    "        self.cov_matrix = covariance_matrix\n",
    "        self.m = np.shape(self.cov_matrix)[0]\n",
    "        self.R = covariance_matrix\n",
    "        return covariance_matrix\n",
    "    \n",
    "\n",
    "    def get_cov_matrix_toeplitz(self, signal):\n",
    "        '''\n",
    "        This generates R tilde of DOI: 10.1109/LSP.2015.2409153 and only stores a column and row, which entirely \n",
    "        defines a Toeplitz matrix\n",
    "        '''\n",
    "        self.ULA_signal = get_ula_signal(self.q, self.idx, signal)\n",
    "        total_size = len(self.ULA_signal)\n",
    "        ULA_signal = self.ULA_signal\n",
    "        \n",
    "        subarray_col = ULA_signal[total_size//2:]\n",
    "        subarray_row = np.conj(subarray_col)\n",
    "        \n",
    "        return subarray_col\n",
    "    \n",
    "    def get_idx(self):\n",
    "        virtual_locations = []\n",
    "        depths = self.depths\n",
    "        q = self.q\n",
    "        list_of_idx = []\n",
    "        difference_matrix = np.zeros((len(depths), len(depths)), dtype=int)\n",
    "        for r, rval in enumerate(depths):\n",
    "            for c, cval in enumerate(depths):\n",
    "                difference_matrix[r][c] = rval-cval\n",
    "        depths0 = difference_matrix.flatten(order='F')\n",
    "        depths0, idx = np.unique(depths0, return_index = True)\n",
    "        new_depths = depths0\n",
    "        list_of_idx.append(idx)\n",
    "\n",
    "        virtual_locations.append(depths0)\n",
    "        for i in range(q-1):\n",
    "            difference_matrix = np.zeros((len(new_depths), len(depths0)), dtype=int)\n",
    "            for r, rval in enumerate(new_depths):\n",
    "                for c, cval in enumerate(depths0):\n",
    "                    difference_matrix[r][c] = rval-cval\n",
    "            new_depths = difference_matrix.flatten(order='F')\n",
    "            new_depths, idx = np.unique(new_depths, return_index = True)\n",
    "            virtual_locations.append(new_depths)\n",
    "\n",
    "            if i<q-2:\n",
    "                list_of_idx.append(idx)\n",
    "\n",
    "        self.virtual_locations = virtual_locations\n",
    "\n",
    "        difference_set = new_depths\n",
    "        a = difference_set\n",
    "        b = np.diff(a)\n",
    "        b = b[:len(b)//2]\n",
    "        try:\n",
    "            start_idx = np.max(np.argwhere(b>1)) + 1\n",
    "            list_of_idx.append(idx[start_idx:-start_idx])\n",
    "        except:\n",
    "            list_of_idx.append(idx)\n",
    "\n",
    "        return list_of_idx\n",
    "\n",
    "    def _get_depths(self, narray, C=1.2):\n",
    "        physLoc = []\n",
    "        n_samples = []\n",
    "\n",
    "        r = (len(narray)-2)//2\n",
    "\n",
    "        for i,m in enumerate(narray):\n",
    "            c = int(np.prod(narray[:i]))\n",
    "            for j in range(m):\n",
    "                physLoc.append(j*c)\n",
    "\n",
    "        physLoc = np.sort(list(set(physLoc)))\n",
    "\n",
    "        for i in range(len(physLoc)):\n",
    "            x = int((np.ceil(C*(len(physLoc)-i)))) # sims_99\n",
    "            n_samples.append(x if x!=0 else 1)\n",
    "        n_samples[0] = n_samples[0] * 2\n",
    "        return physLoc, n_samples\n",
    "    \n",
    "    def estimate_signal(self, n_samples, theta, eta=0.0):\n",
    "        depths = self.depths\n",
    "        signals = np.zeros(len(depths), dtype = np.complex128)\n",
    "        self.measurements = np.zeros(len(depths), dtype=np.double)\n",
    "        for i,n in enumerate(depths):\n",
    "            # Get the exact measuremnt probabilities\n",
    "            p0 = P0(n, theta)\n",
    "            p1 = P1(n, theta)\n",
    "\n",
    "            p0x = P0x(n,theta)\n",
    "            p1x = P1x(n,theta)\n",
    "\n",
    "            # Get the \"noisy\" probabilities by sampling and adding a bias term that pushes towards 50/50 mixture\n",
    "            eta_n = (1.0-eta)**(n+1) # The error at depth n increases as more queries are implemented\n",
    "            p0_estimate = np.random.binomial(n_samples[i], eta_n*p0 + (1.0-eta_n)*0.5)/n_samples[i]\n",
    "            p1_estimate = 1.0 - p0_estimate\n",
    "            p0x_estimate = np.random.binomial(n_samples[i], eta_n*p0x + (1.0-eta_n)*0.5)/n_samples[i]\n",
    "            p1x_estimate = 1.0 - p0x_estimate\n",
    "\n",
    "            self.measurements[i] = p0_estimate\n",
    "            \n",
    "            # Estimate theta\n",
    "            theta_estimated = np.arctan2(p0x_estimate - p1x_estimate, p0_estimate - p1_estimate)\n",
    "            \n",
    "            # Store this to determine angle at theta = 0 or pi/2\n",
    "            if i==0:\n",
    "                self.p0mp1 = p0_estimate - p1_estimate\n",
    "\n",
    "            # Compute f(n) - Eq. 3\n",
    "            fi_estimate = np.exp(1.0j*theta_estimated)\n",
    "            signals[i] = fi_estimate\n",
    "        \n",
    "        return signals    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = lambda n, theta: np.cos((2*n+1)*theta)**2\n",
    "P1 = lambda n, theta: np.sin((2*n+1)*theta)**2\n",
    "\n",
    "def estimate_signal(depths, n_samples, theta):\n",
    "        signals = np.zeros(len(depths), dtype = np.complex128)\n",
    "        measurements = np.zeros(len(depths))\n",
    "        for i,n in enumerate(depths):\n",
    "            # Get the exact measuremnt probabilities\n",
    "            p0 = P0(n, theta)\n",
    "            p1 = P1(n, theta)\n",
    "\n",
    "            p0x = P0x(n,theta)\n",
    "            p1x = P1x(n,theta)\n",
    "            \n",
    "            p0_estimate = np.random.binomial(n_samples[i], p0)/n_samples[i]\n",
    "            p1_estimate = 1 - p0_estimate\n",
    "\n",
    "            p0x_estimate = np.random.binomial(n_samples[i], p0x)/n_samples[i]\n",
    "            p1x_estimate = 1.0 - p0x_estimate\n",
    "            measurements[i] = p0_estimate\n",
    "            \n",
    "            # Estimate theta\n",
    "            theta_estimated = np.arctan2(p0x_estimate - p1x_estimate, p0_estimate - p1_estimate)\n",
    "\n",
    "            # Compute f(n) - Eq. 3\n",
    "            fi_estimate = np.exp(1.0j*theta_estimated)\n",
    "            signals[i] = fi_estimate\n",
    "         \n",
    "        return signals, measurements\n",
    "\n",
    "from scipy.stats import binom\n",
    "\n",
    "def apply_correction(ula_signal, measurements, theta_est, theta):\n",
    "    theta_est = np.abs(theta_est)\n",
    "    p_exact = np.cos((2*ula_signal.depths+1)*(theta))**2\n",
    "    p_neg = np.cos((2*ula_signal.depths+1)*(-theta))**2\n",
    "    p_o2 = np.cos((2 * ula_signal.depths + 1) * (theta_est/2.0)) ** 2\n",
    "    p_o4 = np.cos((2 * ula_signal.depths + 1) * (theta_est/4.0)) ** 2\n",
    "    p_same = np.cos((2*ula_signal.depths+1)*(theta_est))**2\n",
    "    p_s2 = np.cos((2 * ula_signal.depths + 1) * (np.pi/2-theta_est)) ** 2\n",
    "    p_s4 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 4 - theta_est)) ** 2\n",
    "    p_s2_o2 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 2 - theta_est/2)) ** 2\n",
    "    p_s4_o2 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 4 - theta_est/2)) ** 2\n",
    "\n",
    "    l_exact = np.sum(\n",
    "        np.log([1e-75 + binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk],\n",
    "                                  p_exact[kk]) for kk in\n",
    "                range(len(ula_signal.n_samples))]))\n",
    "    l_neg = np.sum(\n",
    "        np.log([1e-75 + binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk],\n",
    "                                  p_neg[kk]) for kk in\n",
    "                range(len(ula_signal.n_samples))]))\n",
    "    l_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk]*measurements[kk], ula_signal.n_samples[kk], p_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_o4 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_o4[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_same = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_same[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s4 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s4[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s2_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s2_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s4_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s4_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    \n",
    "    which_correction = np.argmax([l_same, l_s2, l_s4, l_o2, l_o4, l_s2_o2, l_s4_o2])\n",
    "    if which_correction == 1:\n",
    "        theta_est = np.pi/2.0 - theta_est\n",
    "    elif which_correction == 2:\n",
    "        theta_est = np.pi/4.0 - theta_est\n",
    "    elif which_correction == 3:\n",
    "        theta_est = theta_est/2\n",
    "    elif which_correction == 4:\n",
    "        theta_est = theta_est/4\n",
    "    elif which_correction == 5:\n",
    "        theta_est = np.pi / 2.0 - 0.5 * theta_est\n",
    "    elif which_correction == 6:\n",
    "        theta_est = np.pi / 4.0 - 0.5 * theta_est\n",
    "    # elif which_correction == 7:\n",
    "    #     theta_est = -theta_est\n",
    "\n",
    "    # print(f'FINAL ANGLE FOUND: {theta_est, theta}')\n",
    "\n",
    "    return np.abs(theta_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few variations:\n",
      "(1, -1, -1, -1)\n",
      "(1, -1, -1, 1)\n",
      "(1, -1, 1, -1)\n",
      "(1, -1, 1, 1)\n",
      "(1, 1, -1, -1)\n",
      "(1, 1, -1, 1)\n",
      "(1, 1, 1, -1)\n",
      "(1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from typing import List\n",
    "\n",
    "def generate_all_sign_variations(signs_array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate all possible sign variations by changing signs at all possible pairs of positions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signs_array : numpy.ndarray\n",
    "        Original array of signs (typically containing 1 or -1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of all possible sign variation arrays\n",
    "    \"\"\"\n",
    "    # Get all possible unique pairs of positions\n",
    "    n = len(signs_array)\n",
    "    position_pairs = list(itertools.combinations(range(n), 2))\n",
    "    \n",
    "    # Will store all variations\n",
    "    all_variations = []\n",
    "    \n",
    "    # Iterate through all possible position pairs\n",
    "    for pos1, pos2 in position_pairs:\n",
    "        # Generate variations for this pair of positions\n",
    "        pair_variations = generate_pair_variations(signs_array, pos1, pos2)\n",
    "        all_variations.extend(pair_variations)\n",
    "    \n",
    "    return all_variations\n",
    "\n",
    "def generate_adjacent_sign_variations(signs_array: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate sign variations by sliding a two-position window across the array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signs_array : numpy.ndarray\n",
    "        Original array of signs (typically containing 1 or -1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of all possible sign variation arrays\n",
    "    \"\"\"\n",
    "    # Total length of the array\n",
    "    n = len(signs_array)\n",
    "    \n",
    "    # Will store all variations\n",
    "    all_variations = []\n",
    "    \n",
    "    # Iterate through adjacent position pairs \n",
    "    # (0,1), (1,2), (2,3), ... until the second-to-last pair\n",
    "    for pos1 in range(1, n - size + 1):\n",
    "        pos = [pos1 + i for i in range(size)]\n",
    "        \n",
    "        # Generate variations for this pair of adjacent positions\n",
    "        pair_variations = generate_pair_variations(signs_array, pos)\n",
    " \n",
    "        all_variations.extend(pair_variations)\n",
    "    \n",
    "    return all_variations\n",
    "\n",
    "def generate_pair_variations(signs_array: np.ndarray, pos: List[int]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate sign variations for a specific pair of positions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signs_array : numpy.ndarray\n",
    "        Original array of signs\n",
    "    pos1 : int\n",
    "        First position to modify\n",
    "    pos2 : int\n",
    "        Second position to modify\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[numpy.ndarray]\n",
    "        List of sign variation arrays\n",
    "    \"\"\"\n",
    "    # Validate input positions\n",
    "    # if pos1 < 0 or pos2 < 0 or pos1 >= len(signs_array) or pos2 >= len(signs_array):\n",
    "    #     raise ValueError(\"Positions must be within the array bounds\")\n",
    "    \n",
    "    # Generate all possible sign combinations for the two positions\n",
    "    sign_combinations = list(itertools.product([-1, 1], repeat=len(pos)))\n",
    "    \n",
    "    # Create variations\n",
    "    variations = []\n",
    "    for combo in sign_combinations:\n",
    "        # Create a copy of the original array\n",
    "        variation = signs_array.copy()\n",
    "        \n",
    "        # Modify the two specified positions\n",
    "        for i in range(len(combo)):\n",
    "            variation[pos[i]] *= combo[i]\n",
    "        \n",
    "        variations.append(tuple(variation))\n",
    "    \n",
    "    return variations\n",
    "\n",
    "\n",
    "# Example array\n",
    "signs = np.array([1]*4)\n",
    "\n",
    "# Generate variations for all position pairs\n",
    "all_variations = generate_adjacent_sign_variations(signs, 3)\n",
    "\n",
    "# print(\"Original array:\", signs)\n",
    "# print(f\"\\nTotal variations: {len(all_variations)}\")\n",
    "# print(f'4*len(signs)^2: {4*len(signs)**2}')\n",
    "# print(f'2^len(signs): {2**len(signs)}')\n",
    "\n",
    "print(\"\\nFirst few variations:\")\n",
    "for var in all_variations:  # Print first 10 variations\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_eig(lp, cos_signal, abs_sin, ula_signal, esprit):   \n",
    "    signal = cos_signal + 1.0j * lp * abs_sin\n",
    "    R = ula_signal.get_cov_matrix_toeplitz(signal)\n",
    "    _, _ = esprit.estimate_theta_toeplitz(R)\n",
    "    eigs = np.abs(esprit.eigs)[:2]\n",
    "    obj = eigs[1] - eigs[0]\n",
    "\n",
    "    return obj\n",
    "\n",
    "def objective_function(lp, cos_signal, abs_sin, ula_signal, esprit):\n",
    "    signal = cos_signal + 1.0j * lp * abs_sin\n",
    "    R = ula_signal.get_cov_matrix_toeplitz(signal)\n",
    "    theta_est, _ = esprit.estimate_theta_toeplitz(R)\n",
    "\n",
    "    theta_est = apply_correction(ula_signal, ula_signal.measurements, theta_est, theta_est)\n",
    "\n",
    "    # print(f'2*theta_est: {2*theta_est}')\n",
    "    p_same = np.cos((2 * ula_signal.depths + 1) * (theta_est)) ** 2\n",
    "\n",
    "    obj = -np.sum(\n",
    "        np.log(\n",
    "            [1e-75 + binom.pmf(ula_signal.n_samples[kk] * ula_signal.measurements[kk], ula_signal.n_samples[kk], p_same[kk]) for kk\n",
    "             in\n",
    "             range(len(ula_signal.n_samples))]))\n",
    "\n",
    "\n",
    "    # eigs = np.abs(esprit.eigs)[:2]\n",
    "    # obj = eigs[1] - eigs[0]\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the distribution of the signs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "narray = [2,2,2,2,2,3]\n",
    "\n",
    "def get_heavy_signs(narray, steps):\n",
    "\n",
    "    thetas = np.linspace(np.arcsin(0.09), np.arcsin(0.91), steps)\n",
    "    avals = np.sin(thetas)\n",
    "\n",
    "    ula_signal = TwoqULASignal(M=narray, C=3)\n",
    "\n",
    "    num_mc = 1000\n",
    "\n",
    "    sign_distributions = {}\n",
    "\n",
    "    for a,theta in zip(avals,thetas):\n",
    "        # theta = np.arcsin(a)\n",
    "        distr = {}\n",
    "        for i in range(num_mc):\n",
    "            signal, _ = estimate_signal(ula_signal.depths, ula_signal.n_samples, theta)\n",
    "                # print(measurements)\n",
    "            signs = tuple(np.sign(np.imag(signal)))\n",
    "            # print(signs)\n",
    "\n",
    "            distr[signs] = distr.get(signs, 0.0) + 1/num_mc\n",
    "        sign_distributions[str(a)] = distr\n",
    "\n",
    "    def get_signs(sign_distribution):\n",
    "        # Sort the dictionary by values in descending order\n",
    "        sorted_data = dict(sorted(sign_distribution.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Create a new dictionary with cumulative sum close to 0.68\n",
    "        cumulative_dict = {}\n",
    "        cumulative_sum = 0.0\n",
    "\n",
    "        for key, value in sorted_data.items():\n",
    "            cumulative_dict[key] = value\n",
    "            if cumulative_sum + value > 0.9:\n",
    "                break\n",
    "            cumulative_sum += value\n",
    "\n",
    "        # Output the result\n",
    "        return cumulative_dict\n",
    "    \n",
    "    ret_dict = {}\n",
    "    for a in avals:\n",
    "        distr = get_signs(sign_distributions[str(a)])\n",
    "        ret_dict[str(a)] = distr\n",
    " \n",
    "    # Normalize the values\n",
    "    normalized_data = {}\n",
    "    for key, sub_dict in ret_dict.items():\n",
    "        total_sum = sum(sub_dict.values())\n",
    "        normalized_data[key] = {k: v / total_sum for k, v in sub_dict.items()}\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "normalized_data = get_heavy_signs(narray, 10)\n",
    "\n",
    "# normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Taking a random sample based on the normalized probabilities\n",
    "def sample_from_normalized(data, sample_size=1):\n",
    "    keys = list(map(str, list(data.keys())))\n",
    "    probabilities = list(data.values())\n",
    "    sampled_keys = np.random.choice(a=keys, size=sample_size, p=probabilities)\n",
    "    return [ast.literal_eval(t) for t in sampled_keys]\n",
    "avals = list(normalized_data.keys())\n",
    "# Get one random sample\n",
    "sampled = sample_from_normalized(normalized_data[avals[0]], sample_size=3)\n",
    "# print(\"Sampled keys:\",sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the distribution of signs so we don't have to try all possible signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "correct signs: [ 1.  1.  1.  1. -1.  1.  1.  1.  1.]\n",
      "rough estimate a: 0.18910752115495127\n",
      "avals to use: ['0.13981296510949756', '0.15634532387578837', '0.1728339924311597', '0.18927436306884374', '0.20566184157877418', '0.22199184853142423']\n",
      "number of signs Hamming distance two: 114\n",
      "debug\n",
      "[(1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0), (1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0), (1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0), (1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0), (1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, 1.0, -1.0, 0.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0), (1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0), (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0), (1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0), (1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0), (1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0), (1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 0.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0), (1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0), (1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0), (1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0), (1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0), (1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0), (1.0, 1.0, 1.0, -1.0, -1.0, 0.0, 1.0, -1.0, -1.0), (1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0), (1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0), (1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0), (1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0), (1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0), (1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0), (1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0), (1.0, 1.0, -1.0, 1.0, 1.0, 0.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 0.0, 1.0, -1.0, 1.0), (1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0), (1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0), (1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0), (1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0), (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0), (1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0), (1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0), (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0), (1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0), (1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0), (1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0), (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0), (1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0), (1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0), (1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0), (1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0), (1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0), (1.0, -1.0, -1.0, -1.0, 1.0, 0.0, 1.0, -1.0, -1.0), (1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0), (1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0), (1.0, -1.0, 1.0, -1.0, 1.0, 0.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 0.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, -0.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0), (1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0), (1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0), (1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0), (1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0), (1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0), (1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0), (1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 0.0, -1.0, 1.0, -1.0), (1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, -1.0, 1.0, 0.0, 1.0, 1.0, 1.0), (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0), (1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0), (1.0, 1.0, -1.0, -1.0, 1.0, 0.0, 1.0, -1.0, -1.0), (1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0), (1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0)]\n",
      "current objective: 101.5852036831245\n",
      "current best signs: (1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0)\n",
      "current objective: 12.189443708864829\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0)\n",
      "current objective: 12.178888471805434\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0)\n",
      "current objective: 12.172685946848544\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0)\n",
      "current objective: 12.170060278421333\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0)\n",
      "current objective: 12.170060278421332\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0)\n",
      "current objective: 12.169852128294572\n",
      "current best signs: (1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0)\n",
      "[ 1. -1.  1.  1.  1. -1.  1. -1.  1.]\n",
      "basin obj: -71.49676886330401\n",
      "true obj: -397.45165984862285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'theta_est': 0.10014050463580293,\n",
       " 'theta_est1': 0.10012026433053867,\n",
       " 'error': 2.6781641200374073e-05,\n",
       " 'error1': 4.692056561073077e-05,\n",
       " 'queries': 2600,\n",
       " 'depth': 128,\n",
       " 'true_obj': -397.45165984862285,\n",
       " 'basin_obj': -71.49676886330401,\n",
       " 'x_star': array([ 1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "a = 0.1\n",
    "print(a)\n",
    "theta = np.arcsin(a)\n",
    "\n",
    "narray = [2,2,2,2,2,2,2,2]\n",
    "heavy_signs = get_heavy_signs(narray, len(narray)**2)\n",
    "\n",
    "ula_signal = TwoqULASignal(M=narray, C=5)\n",
    "esprit = ESPIRIT()\n",
    "\n",
    "def sample_signs(heavy_signs, sample_size=3):\n",
    "    \"\"\"\n",
    "    Sample a set of sign variations from a learned sign distribution.\n",
    "\n",
    "    Args:\n",
    "        heavy_signs (dict): A dictionary where the keys are strings representing float values,\n",
    "                           and the values are dictionaries with keys as sign vectors and values\n",
    "                           as their corresponding probabilities.\n",
    "        sample_size (int, optional): The number of sign variations to sample. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the same as the keys in `heavy_signs`,\n",
    "              and the values are lists of sampled sign variations.\n",
    "    \"\"\"\n",
    "    avals = list(heavy_signs.keys())\n",
    "    signs_to_try = {}\n",
    "    for a in avals:\n",
    "        signs_to_try[a] = list(set(sample_from_normalized(heavy_signs[a], sample_size=sample_size)))\n",
    "\n",
    "    return signs_to_try\n",
    "\n",
    "def avals_to_usef(a0, avals, L=3):\n",
    "    \"\"\"\n",
    "    Find the `L` values in `avals` that are closest to the given `a0` value.\n",
    "\n",
    "    Args:\n",
    "        a0 (float): The reference value to find the closest `L` values for.\n",
    "        avals (list): A list of float values representing the keys in `heavy_signs`.\n",
    "        L (int, optional): The number of closest values to return. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of string representations of the `L` closest values to `a0` in `avals`.\n",
    "    \"\"\"\n",
    "    avals = list(map(float, avals))\n",
    "\n",
    "    left = 0\n",
    "    right = len(avals)\n",
    "    \n",
    "    while left < right:\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if avals[mid] <= a0:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid\n",
    "\n",
    "    if left >= 4:\n",
    "        avals_to_use = list(map(str, avals[left-L: left+L]))\n",
    "    if left >= 3:\n",
    "        avals_to_use = list(map(str, avals[left-3: left+L]))\n",
    "    elif left>=2:\n",
    "        avals_to_use = list(map(str, avals[left-2: left+L]))\n",
    "    elif left>=1:\n",
    "        avals_to_use = list(map(str, avals[left-1: left+L]))\n",
    "    else:\n",
    "        avals_to_use = list(map(str, avals[left: left+L]))\n",
    "    \n",
    "    return avals_to_use\n",
    "\n",
    "def all_signs_to_try(avals_to_use, signs_to_try, adjacency=2):\n",
    "    \"\"\"\n",
    "    Generate all possible sign variations within a Hamming distance of 2 from the given `avals_to_use`.\n",
    "\n",
    "    Args:\n",
    "        avals_to_use (list): A list of string representations of float values.\n",
    "        signs_to_try (dict): A dictionary where the keys are string representations of float values,\n",
    "                            and the values are lists of sign variations.\n",
    "        adjacency (int, optional): The maximum Hamming distance to consider for the sign variations.\n",
    "                                  Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of all possible sign variations within the specified Hamming distance.\n",
    "    \"\"\"\n",
    "    all_signs = []\n",
    "    for a in avals_to_use:\n",
    "        for x in signs_to_try[a]:\n",
    "            hamming_distance_two_signs = generate_adjacent_sign_variations(np.array(x), adjacency)\n",
    "            all_signs.extend(hamming_distance_two_signs)\n",
    "    all_signs = list(set(all_signs))\n",
    "    \n",
    "    return all_signs\n",
    "\n",
    "def minimize_obj(all_signs, cos_signal, abs_sin, ula_signal, esprit, disp):\n",
    "    \"\"\"\n",
    "    Find the sign variation that minimizes the objective function.\n",
    "\n",
    "    Args:\n",
    "        all_signs (list): A list of all possible sign variations to consider.\n",
    "        cos_signal (numpy.ndarray): The real part of the estimated signal.\n",
    "        abs_sin (numpy.ndarray): The absolute value of the imaginary part of the estimated signal.\n",
    "        ula_signal (ULASignal): An instance of the ULASignal class, containing information about the signal.\n",
    "        esprit (ESPRIT): An instance of the ESPRIT class, used for estimating the signal parameters.\n",
    "        disp (bool): If True, prints the current objective and the current best sign variation.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The sign variation that minimizes the objective function.\n",
    "    \"\"\"\n",
    "    # obj = 1.0\n",
    "    x_star = np.array(all_signs[0])\n",
    "    obj = objective_function(x_star, cos_signal, abs_sin, ula_signal, esprit)\n",
    "    for x in all_signs:\n",
    "        curr_obj = objective_function(np.array(x), cos_signal, abs_sin, ula_signal, esprit)\n",
    "        if curr_obj < obj:\n",
    "            if disp:\n",
    "                print(f'current objective: {curr_obj}')\n",
    "                print(f'current best signs: {x}')\n",
    "            obj = curr_obj\n",
    "            x_star = np.array(x)\n",
    "\n",
    "    return x_star\n",
    "\n",
    "def csae_with_local_minimization(theta, ula_signal, esprit, heavy_signs, sample=False, correction=False, optimize=False, disp=False):\n",
    "    \"\"\"\n",
    "    Perform CSAE (Compressive Sensing Angle Estimation) with local minimization.\n",
    "\n",
    "    Args:\n",
    "        theta (float): The true angle of the signal.\n",
    "        ula_signal (ULASignal): An instance of the ULASignal class, containing information about the signal.\n",
    "        esprit (ESPRIT): An instance of the ESPRIT class, used for estimating the signal parameters.\n",
    "        heavy_signs (dict): A dictionary where the keys are strings representing float values,\n",
    "                           and the values are dictionaries with keys as sign vectors and values\n",
    "                           as their corresponding probabilities.\n",
    "        sample (bool, optional): If True, samples sign variations from the learned sign distribution.\n",
    "                                If False, uses the learned sign distribution directly. Defaults to False.\n",
    "        correction (bool, optional): If True, applies a correction to the estimated angle. Defaults to False.\n",
    "        optimize (bool, optional): If True, uses a sliding window approach to find the best sign variation.\n",
    "                                  If False, uses the learned sign distribution directly. Defaults to False.\n",
    "        disp (bool, optional): If True, prints additional information during the process. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the estimated angles, errors, number of queries, maximum depth,\n",
    "              and other relevant information.\n",
    "    \"\"\"\n",
    "\n",
    "    depths = ula_signal.depths\n",
    "    n_samples = ula_signal.n_samples\n",
    "    csignal, measurements = estimate_signal(depths, n_samples, theta)\n",
    "    ula_signal.measurements = measurements\n",
    "\n",
    "    cos_signal = np.real(csignal)\n",
    "\n",
    "    correct_signs = np.sign(np.imag(csignal))\n",
    "    if disp:\n",
    "        print(f'correct signs: {correct_signs}')\n",
    "    abs_sin = np.abs(np.imag(csignal))\n",
    "\n",
    "    if sample:\n",
    "        # step 1: sample signs from learned sign distribution\n",
    "        signs_to_try = sample_signs(heavy_signs=heavy_signs, sample_size=3)\n",
    "        avals = list(heavy_signs.keys())\n",
    "\n",
    "        if optimize:\n",
    "            # step 2: using rough estimate where the amplitude is, use the a-values around that estimate\n",
    "            a0 = np.sqrt(0.5 - 0.5 * cos_signal[0])\n",
    "            avals_to_use = avals_to_usef(a0, avals, L=3)\n",
    "\n",
    "            if disp:\n",
    "                print(f'rough estimate a: {a0}')\n",
    "                print(f'avals to use: {avals_to_use}')\n",
    "\n",
    "            # step 3: now vary the signs in a sliding window of size \"adjacency\"\n",
    "            all_signs = all_signs_to_try(avals_to_use, signs_to_try, adjacency=2)\n",
    "\n",
    "            if disp:\n",
    "                print(f'number of signs Hamming distance two: {len(all_signs)}')\n",
    "\n",
    "            # step 4: try all the signs pick the ones that minimize the objective function\n",
    "            if disp:\n",
    "                print('debug')\n",
    "                print(all_signs)\n",
    "            x_star = minimize_obj(all_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "\n",
    "            # step 5 (optional): do one more sweep\n",
    "            hamming_distance_one_signs = list(set(generate_adjacent_sign_variations(x_star, 1)))\n",
    "            x_star = minimize_obj(hamming_distance_one_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "        \n",
    "        else:\n",
    "            # here we don't vary the signs using a sliding window, but directly use the learned sign distribution (poor performance)\n",
    "            all_signs = []\n",
    "            for a in avals:\n",
    "                all_signs.extend(heavy_signs[a])\n",
    "            all_signs = list(set(all_signs))\n",
    "            x_star = minimize_obj(all_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "\n",
    "    else:\n",
    "        avals = list(heavy_signs.keys())\n",
    "        signs_to_try = {}\n",
    "        for a in avals:\n",
    "            signs_to_try[a] = heavy_signs[a]\n",
    "\n",
    "        if optimize:\n",
    "            # step 2: using rough estimate where the amplitude is, use the a-values around that estimate\n",
    "            a0 = np.sqrt(0.5 - 0.5 * cos_signal[0])\n",
    "            avals_to_use = avals_to_usef(a0, avals, L=3)\n",
    "\n",
    "            if disp:\n",
    "                print(f'rough estimate a: {a0}')\n",
    "                print(f'avals to use: {avals_to_use}')\n",
    "\n",
    "            # step 3: now vary the signs in a sliding window of size \"adjacency\"\n",
    "            all_signs = all_signs_to_try(avals_to_use, signs_to_try, adjacency=2)\n",
    "\n",
    "            if disp:\n",
    "                print(f'number of signs Hamming distance two: {len(all_signs)}')\n",
    "\n",
    "            # step 4: try all the signs pick the ones that minimize the objective function\n",
    "            x_star = minimize_obj(all_signs, cos_signal, abs_sin, ula_signal, esprit, disp, measu)\n",
    "\n",
    "            # step 5 (optional): do one more sweep\n",
    "            hamming_distance_one_signs = list(set(generate_adjacent_sign_variations(x_star, 1)))\n",
    "            x_star = minimize_obj(hamming_distance_one_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "        \n",
    "        else:\n",
    "            # here we don't vary the signs using a sliding window, but directly use the learned sign distribution (poor performance)\n",
    "            all_signs = []\n",
    "            for a in avals:\n",
    "                all_signs.extend(heavy_signs[a])\n",
    "            all_signs = list(set(all_signs))\n",
    "            x_star = minimize_obj(all_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "\n",
    "\n",
    "    # Optimization is done.\n",
    "\n",
    "    if disp:\n",
    "        print(x_star)\n",
    "        \n",
    "    signal = cos_signal + 1.0j * x_star * abs_sin\n",
    "    R = ula_signal.get_cov_matrix_toeplitz(signal)\n",
    "    theta_est, _ = esprit.estimate_theta_toeplitz(R)\n",
    "    theta_est = np.abs(theta_est)\n",
    "    eigs = np.abs(esprit.eigs)[:2]\n",
    "    basin_obj = eigs[1] - eigs[0]\n",
    "    if correction:\n",
    "        theta_est = apply_correction(ula_signal, measurements, theta_est, theta) #apply correction\n",
    "    if disp:\n",
    "        print(f'basin obj: {basin_obj}')\n",
    "\n",
    "    cR = ula_signal.get_cov_matrix_toeplitz(csignal)\n",
    "    theta_est1, _ = esprit.estimate_theta_toeplitz(cR)\n",
    "    eigs = np.abs(esprit.eigs)[:2]\n",
    "    true_obj = eigs[1] - eigs[0]\n",
    "    if disp:\n",
    "        print(f'true obj: {true_obj}')\n",
    "\n",
    "    # compute queries required\n",
    "    num_queries = np.sum(np.array(ula_signal.depths)*np.array(ula_signal.n_samples)) + ula_signal.n_samples[0]\n",
    "    max_single_query = np.max(ula_signal.depths)\n",
    "\n",
    "    ret_dict = {'theta_est': theta_est, 'theta_est1': theta_est1, \n",
    "                'error': np.abs(np.sin(theta)-np.sin(theta_est)), \n",
    "                'error1': np.abs(np.sin(theta)-np.sin(theta_est1)), \n",
    "                'queries': num_queries, 'depth': max_single_query,\n",
    "                'true_obj': true_obj, 'basin_obj': basin_obj,\n",
    "                'x_star': x_star}\n",
    "    \n",
    "    return ret_dict\n",
    "\n",
    "csae_with_local_minimization(theta, ula_signal, esprit, heavy_signs, sample=True, correction=True, optimize=True, disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some statistics now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['0.10663566752567158',\n",
       "  '0.12324153604814797',\n",
       "  '0.13981296510949756',\n",
       "  '0.15634532387578837',\n",
       "  '0.1728339924311597',\n",
       "  '0.18927436306884374'],\n",
       " ['0.09',\n",
       "  '0.10663566752567158',\n",
       "  '0.12324153604814797',\n",
       "  '0.13981296510949756',\n",
       "  '0.15634532387578837',\n",
       "  '0.1728339924311597',\n",
       "  '0.18927436306884374',\n",
       "  '0.20566184157877418',\n",
       "  '0.22199184853142423',\n",
       "  '0.23825982055751374',\n",
       "  '0.2544612116232283',\n",
       "  '0.2705914943005945',\n",
       "  '0.286646161032655',\n",
       "  '0.3026207253930916',\n",
       "  '0.318510723339943',\n",
       "  '0.33431171446306707',\n",
       "  '0.35001928322499964',\n",
       "  '0.3656290401948629',\n",
       "  '0.3811366232749777',\n",
       "  '0.3965376989198384',\n",
       "  '0.41182796334710836',\n",
       "  '0.42700314374029846',\n",
       "  '0.44205899944279253',\n",
       "  '0.4569913231428852',\n",
       "  '0.47179594204950215',\n",
       "  '0.4864687190582737',\n",
       "  '0.5010055539076351',\n",
       "  '0.5154023843246325',\n",
       "  '0.5296551871601116',\n",
       "  '0.5437599795129751',\n",
       "  '0.5577128198431908',\n",
       "  '0.5715098090732432',\n",
       "  '0.5851470916777191',\n",
       "  '0.5986208567607206',\n",
       "  '0.6119273391208092',\n",
       "  '0.6250628203031795',\n",
       "  '0.6380236296387691',\n",
       "  '0.650806145270016',\n",
       "  '0.6634067951629752',\n",
       "  '0.6758220581055129',\n",
       "  '0.6880484646912971',\n",
       "  '0.7000825982893134',\n",
       "  '0.7119210959986316',\n",
       "  '0.7235606495881572',\n",
       "  '0.7349980064211071',\n",
       "  '0.7462299703639479',\n",
       "  '0.7572534026795463',\n",
       "  '0.7680652229042787',\n",
       "  '0.7786624097088569',\n",
       "  '0.7890420017426304',\n",
       "  '0.7992010984611247',\n",
       "  '0.8091368609365911',\n",
       "  '0.8188465126513348',\n",
       "  '0.8283273402736051',\n",
       "  '0.8375766944158268',\n",
       "  '0.8465919903749636',\n",
       "  '0.855370708854805',\n",
       "  '0.863910396669976',\n",
       "  '0.8722086674314722',\n",
       "  '0.8802632022135276',\n",
       "  '0.8880717502016333',\n",
       "  '0.8956321293215179',\n",
       "  '0.9029422268489239',\n",
       "  '0.91'],\n",
       " 0.14658716786698373)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0=0.14658716786698373\n",
    "signs_to_try = sample_signs(heavy_signs=heavy_signs, sample_size=3)\n",
    "avals = list(heavy_signs.keys())\n",
    "avals_to_usef(a0, avals, L=3), avals, a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning the distribution of signs...\n",
      "theta = 0.41111546403370536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306ac3504f3e46998b8b310ecf43cc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 1.870e-02, 3.3666359341273115, 0.14962826373899163\n",
      "theta = 1.0363905664439828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acc5554b00245cc9bffb199fda134c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 1.845e-02, 3.3204706104907347, 0.14757647157736598\n",
      "theta = 0.7554209219922304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294ff1b7e15a49abb6f2cdb7d4bc8f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.636e-02, 4.745367913546662, 0.21090524060207388\n",
      "theta = 0.6174118623048501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e012ea6d26495ca123cf6b40921214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.105e-02, 3.788725509785876, 0.16838780043492782\n",
      "theta = 0.2267530819288086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6c7ba04410446587efbfcb3264b6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.117e-02, 3.8109976505300187, 0.16937767335688972\n",
      "theta = 0.2267332789613546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b393ba5d59d048b5b27b33ea26c4012f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.040e-02, 3.6726450571836913, 0.16322866920816406\n",
      "theta = 0.14699569205633\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7068b10706db4807adfb1d0b02155eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.777e-02, 4.997807614338188, 0.22212478285947504\n",
      "theta = 0.9156206760936844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c07aeaf9fff4130b0510a7d84426640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 1.403e-02, 2.525619499046813, 0.11224975551319169\n",
      "theta = 0.6198241234230385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885da932fb894ca18ec32c9447c46f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.288e-02, 4.11798865638288, 0.18302171806146134\n",
      "theta = 0.7294478190327938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5f800060204173ac363da06af86571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 1.749e-02, 3.148315643643301, 0.13992513971748005\n",
      "theta = 0.11673252381145406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3222d27294a84bb7a253a3ea38e443de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 3.377e-02, 6.078100034127692, 0.27013777929456406\n",
      "theta = 1.0673557731834724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b07169d692747cd853c0f9e2191259d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 1.363e-02, 2.4540333338328715, 0.10906814817034984\n",
      "theta = 0.8725241084844789\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc794c7722284667a358ba6fb902d337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 1.569e-02, 2.8234565645122505, 0.1254869584227667\n",
      "theta = 0.2732593578259982\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e301232c42640dca7a62c18552d66d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 1.489e-02, 2.6806425203444593, 0.11913966757086486\n",
      "theta = 0.24799415401854524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4643fa941cbb4cc98a455304e562657c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.631e-02, 4.736160113653244, 0.2104960050512553\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "avals = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# avals = [np.random.uniform(0.1, 0.9)]\n",
    "# avals = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# avals = [0.1, 0.2, 0.3]\n",
    "avals = [np.random.uniform(0.1, 0.9) for _ in range(15)]\n",
    "# narray = [2]*5 + [3]\n",
    "narray = [2]*4\n",
    "\n",
    "print('learning the distribution of signs...')\n",
    "heavy_signs = get_heavy_signs(narray, int(6*len(narray)))\n",
    "num_queries = np.zeros(len(avals), dtype=int)\n",
    "max_single_query = np.zeros(len(avals), dtype=int)\n",
    "\n",
    "num_mc=100\n",
    "thetas = np.zeros((len(avals), num_mc))\n",
    "errors = np.zeros((len(avals), num_mc))\n",
    "thetas1 = np.zeros((len(avals), num_mc))\n",
    "errors1 = np.zeros((len(avals), num_mc))\n",
    "\n",
    "basin_obj = np.zeros((len(avals), num_mc))\n",
    "true_obj = np.zeros((len(avals), num_mc))\n",
    "\n",
    "ula_signal = TwoqULASignal(M=narray, C=5)\n",
    "esprit = ESPIRIT()\n",
    "\n",
    "for j,a in enumerate(avals):\n",
    "    theta = np.arcsin(a)\n",
    "    print(f'theta = {theta}')\n",
    "    disp=False\n",
    "    # if j==4:\n",
    "    #     disp=True\n",
    "    for i in tqdm(range(num_mc)):\n",
    "        res = csae_with_local_minimization(theta, ula_signal, esprit, heavy_signs, sample=True, correction=True, optimize=True, disp=disp)\n",
    "        thetas[j][i] = res['theta_est']\n",
    "        errors[j][i] = res['error']\n",
    "\n",
    "        thetas1[j][i] = res['theta_est1']\n",
    "        errors1[j][i] = res['error1']\n",
    "\n",
    "        basin_obj[j][i] = res['basin_obj']\n",
    "        true_obj[j][i] = res['true_obj']\n",
    "    num_queries[j] = res['queries']\n",
    "    max_single_query[j] = res['depth']\n",
    "\n",
    "    print(f'constant factors query and depth: {np.percentile(errors[j], 95):.3e}, {np.percentile(errors[j], 95) * num_queries[j]}, {np.percentile(errors[j], 95) * max_single_query[j]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14\n",
      "0.2\n",
      "0.25\n",
      "0.17\n",
      "0.14\n",
      "0.16\n",
      "0.2\n",
      "0.15\n",
      "0.26\n",
      "0.06\n",
      "0.32\n",
      "0.53\n",
      "0.51\n",
      "0.19\n",
      "0.27\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(avals)):\n",
    "    print(np.sum(np.where(np.array(basin_obj[i])/np.array(true_obj[i]) >= 0.99, 1, 0))/len(basin_obj[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.16005468, 0.88041744, 0.35395624, 0.33454116, 0.30185703,\n",
       "        0.96348892, 0.23848   , 1.        , 0.09394278, 1.        ,\n",
       "        0.65911071, 0.22377447, 0.16903925, 0.23219455, 0.10760314,\n",
       "        0.12192419, 0.15734696, 1.        , 0.13953991, 0.07784626,\n",
       "        0.40005274, 0.57562107, 0.36711104, 0.10934473, 0.12482727,\n",
       "        1.        , 0.16066305, 1.        , 0.12447119, 1.        ,\n",
       "        0.08967808, 0.27506446, 0.24136119, 0.356828  , 0.73807222,\n",
       "        0.10263149, 1.        , 0.21624037, 0.59955709, 1.        ,\n",
       "        1.        , 0.18585462, 0.09777969, 1.05938911, 0.09269206,\n",
       "        0.25974478, 0.41457943, 0.09774346, 0.6933519 , 1.        ,\n",
       "        1.        , 0.23604582, 0.18619113, 0.57794034, 0.07821842,\n",
       "        0.08173108, 0.78331883, 0.19093583, 0.15863997, 1.        ,\n",
       "        0.89221044, 0.15049113, 0.13158803, 1.        , 0.11066742,\n",
       "        0.9110941 , 0.18541151, 0.62664092, 0.44554577, 0.2018394 ,\n",
       "        0.49087274, 0.09769741, 1.        , 0.38771329, 0.1835986 ,\n",
       "        1.00128893, 0.56062587, 0.68873133, 1.        , 1.        ,\n",
       "        0.51732151, 1.        , 0.34431229, 0.31066046, 0.13507118,\n",
       "        0.10273216, 0.76652369, 0.53750088, 0.20201331, 0.1090643 ,\n",
       "        0.13167391, 0.2675541 , 0.10018187, 0.64907494, 0.38549535,\n",
       "        1.        , 0.12379186, 0.3954525 , 0.16759691, 0.15411294]),\n",
       " 0.2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=6\n",
    "np.array(basin_obj[i])/np.array(true_obj[i]), np.sum(np.where(np.array(basin_obj[i])/np.array(true_obj[i]) >= 0.99, 1, 0))/len(basin_obj[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.14775781, 0.1385406 , 0.13279854, 0.13854673, 0.15059242,\n",
       "        0.13210707, 0.1449994 , 0.14175501, 0.1923323 , 0.14008262,\n",
       "        0.15574162, 0.15507179, 0.13189001, 0.14163192, 0.13663395,\n",
       "        0.13502671, 0.13916812, 0.16010863, 0.13935168, 0.14871157,\n",
       "        0.13694166, 0.15062664, 0.1327524 , 0.1476518 , 0.14788026,\n",
       "        0.14261911, 0.14514006, 0.14270738, 0.15217976, 0.13609439,\n",
       "        0.15013154, 0.15288383, 0.14863345, 0.13855705, 0.17413922,\n",
       "        0.16497217, 0.14532673, 0.13768693, 0.15383773, 0.13431567,\n",
       "        0.15772506, 0.13724876, 0.15408273, 0.13543123, 0.17812518,\n",
       "        0.15844732, 0.15798714, 0.13998335, 0.15912292, 0.14647185,\n",
       "        0.14626734, 0.17314526, 0.15946675, 0.17129441, 0.16978222,\n",
       "        0.14619162, 0.16577349, 0.15085871, 0.14823757, 0.13763132,\n",
       "        0.13055231, 0.15194661, 0.15906938, 0.14488303, 0.16161299,\n",
       "        0.13987151, 0.14435883, 0.16617878, 0.13399038, 0.12504971,\n",
       "        0.14961745, 0.11692923, 0.15720603, 0.13136591, 0.1349779 ,\n",
       "        0.13972914, 0.13459023, 0.17744895, 0.12998326, 0.14655491,\n",
       "        0.22309852, 0.15905365, 0.1294316 , 0.14695029, 0.14364772,\n",
       "        0.14681564, 0.14774058, 0.14953591, 0.14058109, 0.15236272,\n",
       "        0.15287111, 0.14235707, 0.16152823, 0.12936224, 0.15804857,\n",
       "        0.14759124, 0.13966468, 0.15004377, 0.13144812, 0.14757498]),\n",
       " 0.14646688973455957)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=6\n",
    "np.sin(thetas[i]), avals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.04720561, 0.04424114, 0.04239633, 0.04424311, 0.04811811,\n",
       "        0.04217427, 0.04631803, 0.04527452, 0.06160514, 0.04473682,\n",
       "        0.04977672, 0.04956089, 0.04210457, 0.04523494, 0.04362841,\n",
       "        0.04311202, 0.04444284, 0.05118445, 0.04450185, 0.0475126 ,\n",
       "        0.04372729, 0.04812913, 0.04238151, 0.0471715 , 0.04724503,\n",
       "        0.0455524 , 0.04636328, 0.04558078, 0.04862927, 0.04345504,\n",
       "        0.04796972, 0.04885603, 0.04748745, 0.04424643, 0.05571428,\n",
       "        0.05275343, 0.04642333, 0.04396678, 0.0491633 , 0.04288361,\n",
       "        0.05041597, 0.04382597, 0.04924223, 0.04324198, 0.0570032 ,\n",
       "        0.0506488 , 0.05050045, 0.04470491, 0.05086662, 0.04679178,\n",
       "        0.04672597, 0.05539301, 0.05097748, 0.05479493, 0.05430643,\n",
       "        0.04670161, 0.05301206, 0.04820385, 0.04736003, 0.04394891,\n",
       "        0.04167505, 0.04855418, 0.05084936, 0.04628059, 0.05166963,\n",
       "        0.04466895, 0.04611196, 0.05314288, 0.04277912, 0.03990903,\n",
       "        0.04780421, 0.03730507, 0.05024867, 0.04193628, 0.04309634,\n",
       "        0.04462318, 0.04297181, 0.05678446, 0.04149236, 0.04681851,\n",
       "        0.07161717, 0.05084428, 0.04131527, 0.04694574, 0.04588322,\n",
       "        0.0469024 , 0.04720007, 0.04777796, 0.04489707, 0.04868819,\n",
       "        0.04885194, 0.04546813, 0.05164229, 0.041293  , 0.05052026,\n",
       "        0.047152  , 0.04460246, 0.04794146, 0.04196268, 0.04714677]),\n",
       " 0.046790182007957946)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=6\n",
    "thetas[i]/np.pi, np.arcsin(avals[i])/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "\n",
    "# csignal, measurements = estimate_signal(ula_signal.depths, ula_signal.n_samples, np.arcsin(avals[i]))\n",
    "\n",
    "# plt.plot(measurements, 'r', label='measurements')\n",
    "# plt.plot(np.cos((2*ula_signal.depths + 1)*theta2)**2, 'b', label='theta2')\n",
    "# plt.plot(np.cos((2*ula_signal.depths + 1)*theta1)**2, 'k', label='theta1')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth (68%) for 0.400: 8.843e-03, 1.592, 7.075e-02\n",
      "constant factors query and depth (68%) for 0.861: 5.419e-03, 0.975, 4.335e-02\n",
      "constant factors query and depth (68%) for 0.686: 7.632e-03, 1.374, 6.105e-02\n",
      "constant factors query and depth (68%) for 0.579: 1.010e-02, 1.818, 8.079e-02\n",
      "constant factors query and depth (68%) for 0.225: 1.043e-02, 1.878, 8.347e-02\n",
      "constant factors query and depth (68%) for 0.225: 9.398e-03, 1.692, 7.518e-02\n",
      "constant factors query and depth (68%) for 0.146: 1.204e-02, 2.166, 9.628e-02\n",
      "constant factors query and depth (68%) for 0.793: 6.483e-03, 1.167, 5.186e-02\n",
      "constant factors query and depth (68%) for 0.581: 1.077e-02, 1.939, 8.617e-02\n",
      "constant factors query and depth (68%) for 0.666: 7.219e-03, 1.300, 5.776e-02\n",
      "constant factors query and depth (68%) for 0.116: 9.712e-03, 1.748, 7.769e-02\n",
      "constant factors query and depth (68%) for 0.876: 6.561e-03, 1.181, 5.249e-02\n",
      "constant factors query and depth (68%) for 0.766: 8.636e-03, 1.554, 6.909e-02\n",
      "constant factors query and depth (68%) for 0.270: 8.145e-03, 1.466, 6.516e-02\n",
      "constant factors query and depth (68%) for 0.245: 1.077e-02, 1.939, 8.617e-02\n",
      "average constant factor 1.586\n",
      "average error 8.810e-03\n"
     ]
    }
   ],
   "source": [
    "perc = 68\n",
    "avg = 0.0\n",
    "avg_err = 0.0\n",
    "for i in range(len(avals)):\n",
    "    avg += np.percentile(errors[i], perc) * num_queries[i]/len(avals)\n",
    "    avg_err += np.percentile(errors[i], perc)/len(avals)\n",
    "    print(f'constant factors query and depth ({perc}%) for {avals[i]:.3f}: {np.percentile(errors[i], perc):.3e}, {np.percentile(errors[i], perc) * num_queries[i]:.3f}, {np.percentile(errors[i], perc) * max_single_query[i]:.3e}')\n",
    "print(f'average constant factor {avg:0.3f}')\n",
    "print(f'average error {avg_err:0.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth with known signs for 0.400: 2.481e-02, 4.466, 1.985e-01\n",
      "constant factors query and depth with known signs for 0.861: 1.184e-02, 2.131, 9.472e-02\n",
      "constant factors query and depth with known signs for 0.686: 1.836e-02, 3.305, 1.469e-01\n",
      "constant factors query and depth with known signs for 0.579: 1.739e-02, 3.131, 1.392e-01\n",
      "constant factors query and depth with known signs for 0.225: 2.835e-02, 5.103, 2.268e-01\n",
      "constant factors query and depth with known signs for 0.225: 2.449e-02, 4.408, 1.959e-01\n",
      "constant factors query and depth with known signs for 0.146: 3.186e-02, 5.735, 2.549e-01\n",
      "constant factors query and depth with known signs for 0.793: 1.582e-02, 2.847, 1.265e-01\n",
      "constant factors query and depth with known signs for 0.581: 2.258e-02, 4.065, 1.806e-01\n",
      "constant factors query and depth with known signs for 0.666: 1.712e-02, 3.082, 1.370e-01\n",
      "constant factors query and depth with known signs for 0.116: 2.238e-02, 4.028, 1.790e-01\n",
      "constant factors query and depth with known signs for 0.876: 1.395e-02, 2.510, 1.116e-01\n",
      "constant factors query and depth with known signs for 0.766: 1.864e-02, 3.354, 1.491e-01\n",
      "constant factors query and depth with known signs for 0.270: 2.704e-02, 4.867, 2.163e-01\n",
      "constant factors query and depth with known signs for 0.245: 1.684e-02, 3.031, 1.347e-01\n",
      "average constant factor 3.738\n",
      "average error 1.038e-02\n"
     ]
    }
   ],
   "source": [
    "perc = 68\n",
    "avg = 0.0\n",
    "avg_err = 0.0\n",
    "for j in range(len(avals)):\n",
    "    avg += 2*np.percentile(errors1[j], perc) * num_queries[j]/len(avals)\n",
    "    avg_err += np.percentile(errors1[j], perc)/len(avals)\n",
    "    print(f'constant factors query and depth with known signs for {avals[j]:.3f}: {2*np.percentile(errors1[j], perc):.3e}, {2*np.percentile(errors1[j], perc) * num_queries[j]:.3f}, {2*np.percentile(errors1[j], perc) * max_single_query[j]:.3e}')\n",
    "print(f'average constant factor {avg:0.3f}')\n",
    "print(f'average error {avg_err:0.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth (95%) for 0.400: 1.870e-02, 3.367, 1.496e-01\n",
      "constant factors query and depth (95%) for 0.861: 1.845e-02, 3.320, 1.476e-01\n",
      "constant factors query and depth (95%) for 0.686: 2.636e-02, 4.745, 2.109e-01\n",
      "constant factors query and depth (95%) for 0.579: 2.105e-02, 3.789, 1.684e-01\n",
      "constant factors query and depth (95%) for 0.225: 2.117e-02, 3.811, 1.694e-01\n",
      "constant factors query and depth (95%) for 0.225: 2.040e-02, 3.673, 1.632e-01\n",
      "constant factors query and depth (95%) for 0.146: 2.777e-02, 4.998, 2.221e-01\n",
      "constant factors query and depth (95%) for 0.793: 1.403e-02, 2.526, 1.122e-01\n",
      "constant factors query and depth (95%) for 0.581: 2.288e-02, 4.118, 1.830e-01\n",
      "constant factors query and depth (95%) for 0.666: 1.749e-02, 3.148, 1.399e-01\n",
      "constant factors query and depth (95%) for 0.116: 3.377e-02, 6.078, 2.701e-01\n",
      "constant factors query and depth (95%) for 0.876: 1.363e-02, 2.454, 1.091e-01\n",
      "constant factors query and depth (95%) for 0.766: 1.569e-02, 2.823, 1.255e-01\n",
      "constant factors query and depth (95%) for 0.270: 1.489e-02, 2.681, 1.191e-01\n",
      "constant factors query and depth (95%) for 0.245: 2.631e-02, 4.736, 2.105e-01\n",
      "average constant factor 3.751\n",
      "average error 2.084e-02\n"
     ]
    }
   ],
   "source": [
    "perc = 95\n",
    "avg = 0.0\n",
    "avg_err = 0.0\n",
    "for i in range(len(avals)):\n",
    "    avg += np.percentile(errors[i], perc) * num_queries[i]/len(avals)\n",
    "    avg_err += np.percentile(errors[i], perc)/len(avals)\n",
    "    print(f'constant factors query and depth ({perc}%) for {avals[i]:.3f}: {np.percentile(errors[i], perc):.3e}, {np.percentile(errors[i], perc) * num_queries[i]:.3f}, {np.percentile(errors[i], perc) * max_single_query[i]:.3e}')\n",
    "print(f'average constant factor {avg:0.3f}')\n",
    "print(f'average error {avg_err:0.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth with known signs for 0.400: 4.886e-02, 8.796, 3.909e-01\n",
      "constant factors query and depth with known signs for 0.861: 2.967e-02, 5.340, 2.373e-01\n",
      "constant factors query and depth with known signs for 0.686: 3.615e-02, 6.508, 2.892e-01\n",
      "constant factors query and depth with known signs for 0.579: 2.941e-02, 5.293, 2.353e-01\n",
      "constant factors query and depth with known signs for 0.225: 5.021e-02, 9.039, 4.017e-01\n",
      "constant factors query and depth with known signs for 0.225: 4.542e-02, 8.175, 3.633e-01\n",
      "constant factors query and depth with known signs for 0.146: 5.478e-02, 9.860, 4.382e-01\n",
      "constant factors query and depth with known signs for 0.793: 2.711e-02, 4.879, 2.168e-01\n",
      "constant factors query and depth with known signs for 0.581: 4.775e-02, 8.596, 3.820e-01\n",
      "constant factors query and depth with known signs for 0.666: 3.631e-02, 6.536, 2.905e-01\n",
      "constant factors query and depth with known signs for 0.116: 4.484e-02, 8.071, 3.587e-01\n",
      "constant factors query and depth with known signs for 0.876: 2.563e-02, 4.613, 2.050e-01\n",
      "constant factors query and depth with known signs for 0.766: 3.174e-02, 5.714, 2.540e-01\n",
      "constant factors query and depth with known signs for 0.270: 4.709e-02, 8.476, 3.767e-01\n",
      "constant factors query and depth with known signs for 0.245: 5.603e-02, 10.086, 4.483e-01\n",
      "average constant factor 7.332\n",
      "average error 2.037e-02\n"
     ]
    }
   ],
   "source": [
    "perc = 95\n",
    "avg = 0.0\n",
    "avg_err = 0.0\n",
    "for j in range(len(avals)):\n",
    "    avg += 2*np.percentile(errors1[j], perc) * num_queries[j]/len(avals)\n",
    "    avg_err += np.percentile(errors1[j], perc)/len(avals)\n",
    "    print(f'constant factors query and depth with known signs for {avals[j]:.3f}: {2*np.percentile(errors1[j], perc):.3e}, {2*np.percentile(errors1[j], perc) * num_queries[j]:.3f}, {2*np.percentile(errors1[j], perc) * max_single_query[j]:.3e}')\n",
    "print(f'average constant factor {avg:0.3f}')\n",
    "print(f'average error {avg_err:0.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth (99%) for 0.400: 2.347e-02, 4.225, 1.878e-01\n",
      "constant factors query and depth (99%) for 0.861: 2.481e-02, 4.466, 1.985e-01\n",
      "constant factors query and depth (99%) for 0.686: 2.837e-01, 51.066, 2.270e+00\n",
      "constant factors query and depth (99%) for 0.579: 4.414e-02, 7.945, 3.531e-01\n",
      "constant factors query and depth (99%) for 0.225: 3.533e-02, 6.359, 2.826e-01\n",
      "constant factors query and depth (99%) for 0.225: 2.441e-02, 4.394, 1.953e-01\n",
      "constant factors query and depth (99%) for 0.146: 4.617e-02, 8.311, 3.694e-01\n",
      "constant factors query and depth (99%) for 0.793: 1.916e-02, 3.449, 1.533e-01\n",
      "constant factors query and depth (99%) for 0.581: 3.769e-02, 6.784, 3.015e-01\n",
      "constant factors query and depth (99%) for 0.666: 2.066e-02, 3.719, 1.653e-01\n",
      "constant factors query and depth (99%) for 0.116: 3.784e-02, 6.811, 3.027e-01\n",
      "constant factors query and depth (99%) for 0.876: 1.636e-02, 2.944, 1.308e-01\n",
      "constant factors query and depth (99%) for 0.766: 1.760e-02, 3.168, 1.408e-01\n",
      "constant factors query and depth (99%) for 0.270: 1.618e-02, 2.912, 1.294e-01\n",
      "constant factors query and depth (99%) for 0.245: 4.792e-02, 8.626, 3.834e-01\n",
      "average constant factor 8.345\n",
      "average error 4.636e-02\n"
     ]
    }
   ],
   "source": [
    "perc = 99\n",
    "avg = 0.0\n",
    "avg_err = 0.0\n",
    "for i in range(len(avals)):\n",
    "    avg += np.percentile(errors[i], perc) * num_queries[i]/len(avals)\n",
    "    avg_err += np.percentile(errors[i], perc)/len(avals)\n",
    "    print(f'constant factors query and depth ({perc}%) for {avals[i]:.3f}: {np.percentile(errors[i], perc):.3e}, {np.percentile(errors[i], perc) * num_queries[i]:.3f}, {np.percentile(errors[i], perc) * max_single_query[i]:.3e}')\n",
    "print(f'average constant factor {avg:0.3f}')\n",
    "print(f'average error {avg_err:0.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth with known signs for 0.400: 5.561e-02, 10.009, 4.449e-01\n",
      "constant factors query and depth with known signs for 0.861: 5.137e-02, 9.246, 4.109e-01\n",
      "constant factors query and depth with known signs for 0.686: 4.629e-02, 8.332, 3.703e-01\n",
      "constant factors query and depth with known signs for 0.579: 6.906e-02, 12.431, 5.525e-01\n",
      "constant factors query and depth with known signs for 0.225: 6.546e-02, 11.783, 5.237e-01\n",
      "constant factors query and depth with known signs for 0.225: 5.062e-02, 9.111, 4.049e-01\n",
      "constant factors query and depth with known signs for 0.146: 6.784e-02, 12.211, 5.427e-01\n",
      "constant factors query and depth with known signs for 0.793: 4.553e-02, 8.195, 3.642e-01\n",
      "constant factors query and depth with known signs for 0.581: 6.841e-02, 12.313, 5.473e-01\n",
      "constant factors query and depth with known signs for 0.666: 4.665e-02, 8.397, 3.732e-01\n",
      "constant factors query and depth with known signs for 0.116: 7.003e-02, 12.606, 5.603e-01\n",
      "constant factors query and depth with known signs for 0.876: 3.101e-02, 5.582, 2.481e-01\n",
      "constant factors query and depth with known signs for 0.766: 3.724e-02, 6.702, 2.979e-01\n",
      "constant factors query and depth with known signs for 0.270: 6.088e-02, 10.959, 4.870e-01\n",
      "constant factors query and depth with known signs for 0.245: 6.991e-02, 12.584, 5.593e-01\n",
      "average constant factor 10.031\n",
      "average error 2.786e-02\n"
     ]
    }
   ],
   "source": [
    "perc = 99\n",
    "avg = 0.0\n",
    "avg_err = 0.0\n",
    "for j in range(len(avals)):\n",
    "    avg += 2*np.percentile(errors1[j], perc) * num_queries[j]/len(avals)\n",
    "    avg_err += np.percentile(errors1[j], perc)/len(avals)\n",
    "    print(f'constant factors query and depth with known signs for {avals[j]:.3f}: {2*np.percentile(errors1[j], perc):.3e}, {2*np.percentile(errors1[j], perc) * num_queries[j]:.3f}, {2*np.percentile(errors1[j], perc) * max_single_query[j]:.3e}')\n",
    "print(f'average constant factor {avg:0.3f}')\n",
    "print(f'average error {avg_err:0.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
