{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farrokhlabib/Documents/github/csAE/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from signals import *\n",
    "from frequencyestimator import *\n",
    "from scipy.optimize import basinhopping, minimize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "sns.set_context(\"poster\", font_scale = .45, rc={\"grid.linewidth\": 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = lambda n, theta: np.cos((2*n+1)*theta)**2\n",
    "P1 = lambda n, theta: np.sin((2*n+1)*theta)**2\n",
    "\n",
    "def estimate_signal(depths, n_samples, theta):\n",
    "        signals = np.zeros(len(depths), dtype = np.complex128)\n",
    "        measurements = np.zeros(len(depths))\n",
    "        for i,n in enumerate(depths):\n",
    "            # Get the exact measuremnt probabilities\n",
    "            p0 = P0(n, theta)\n",
    "            p1 = P1(n, theta)\n",
    "\n",
    "            p0x = P0x(n,theta)\n",
    "            p1x = P1x(n,theta)\n",
    "            \n",
    "            p0_estimate = np.random.binomial(n_samples[i], p0)/n_samples[i]\n",
    "            p1_estimate = 1 - p0_estimate\n",
    "\n",
    "            p0x_estimate = np.random.binomial(n_samples[i], p0x)/n_samples[i]\n",
    "            p1x_estimate = 1.0 - p0x_estimate\n",
    "            measurements[i] = p0_estimate\n",
    "            \n",
    "            # Estimate theta\n",
    "            theta_estimated = np.arctan2(p0x_estimate - p1x_estimate, p0_estimate - p1_estimate)\n",
    "\n",
    "            # Compute f(n) - Eq. 3\n",
    "            fi_estimate = np.exp(1.0j*theta_estimated)\n",
    "            signals[i] = fi_estimate\n",
    "         \n",
    "        return signals, measurements\n",
    "\n",
    "from scipy.stats import binom\n",
    "\n",
    "def apply_correction(ula_signal, measurements, theta_est, theta):\n",
    "    theta_est = np.abs(theta_est)\n",
    "    p_exact = np.cos((2*ula_signal.depths+1)*(theta))**2\n",
    "    p_neg = np.cos((2*ula_signal.depths+1)*(-theta))**2\n",
    "    p_o2 = np.cos((2 * ula_signal.depths + 1) * (theta_est/2.0)) ** 2\n",
    "    p_o4 = np.cos((2 * ula_signal.depths + 1) * (theta_est/4.0)) ** 2\n",
    "    p_same = np.cos((2*ula_signal.depths+1)*(theta_est))**2\n",
    "    p_s2 = np.cos((2 * ula_signal.depths + 1) * (np.pi/2-theta_est)) ** 2\n",
    "    p_s4 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 4 - theta_est)) ** 2\n",
    "    p_s2_o2 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 2 - theta_est/2)) ** 2\n",
    "    p_s4_o2 = np.cos((2 * ula_signal.depths + 1) * (np.pi / 4 - theta_est/2)) ** 2\n",
    "\n",
    "    l_exact = np.sum(\n",
    "        np.log([1e-75 + binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk],\n",
    "                                  p_exact[kk]) for kk in\n",
    "                range(len(ula_signal.n_samples))]))\n",
    "    l_neg = np.sum(\n",
    "        np.log([1e-75 + binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk],\n",
    "                                  p_neg[kk]) for kk in\n",
    "                range(len(ula_signal.n_samples))]))\n",
    "    l_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk]*measurements[kk], ula_signal.n_samples[kk], p_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_o4 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_o4[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_same = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_same[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s4 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s4[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s2_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s2_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    l_s4_o2 = np.sum(\n",
    "        np.log([1e-75+binom.pmf(ula_signal.n_samples[kk] * measurements[kk], ula_signal.n_samples[kk], p_s4_o2[kk]) for kk in\n",
    "         range(len(ula_signal.n_samples))]))\n",
    "    \n",
    "    which_correction = np.argmax([l_same, l_s2, l_s4, l_o2, l_o4, l_s2_o2, l_s4_o2])\n",
    "    if which_correction == 1:\n",
    "        theta_est = np.pi/2.0 - theta_est\n",
    "    elif which_correction == 2:\n",
    "        theta_est = np.pi/4.0 - theta_est\n",
    "    elif which_correction == 3:\n",
    "        theta_est = theta_est/2\n",
    "    elif which_correction == 4:\n",
    "        theta_est = theta_est/4\n",
    "    elif which_correction == 5:\n",
    "        theta_est = np.pi / 2.0 - 0.5 * theta_est\n",
    "    elif which_correction == 6:\n",
    "        theta_est = np.pi / 4.0 - 0.5 * theta_est\n",
    "    # elif which_correction == 7:\n",
    "    #     theta_est = -theta_est\n",
    "\n",
    "    # print(f'FINAL ANGLE FOUND: {theta_est, theta}')\n",
    "\n",
    "    return np.abs(theta_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few variations:\n",
      "(1, -1, -1, -1)\n",
      "(1, -1, -1, 1)\n",
      "(1, -1, 1, -1)\n",
      "(1, -1, 1, 1)\n",
      "(1, 1, -1, -1)\n",
      "(1, 1, -1, 1)\n",
      "(1, 1, 1, -1)\n",
      "(1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from typing import List\n",
    "\n",
    "def generate_all_sign_variations(signs_array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate all possible sign variations by changing signs at all possible pairs of positions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signs_array : numpy.ndarray\n",
    "        Original array of signs (typically containing 1 or -1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of all possible sign variation arrays\n",
    "    \"\"\"\n",
    "    # Get all possible unique pairs of positions\n",
    "    n = len(signs_array)\n",
    "    position_pairs = list(itertools.combinations(range(n), 2))\n",
    "    \n",
    "    # Will store all variations\n",
    "    all_variations = []\n",
    "    \n",
    "    # Iterate through all possible position pairs\n",
    "    for pos1, pos2 in position_pairs:\n",
    "        # Generate variations for this pair of positions\n",
    "        pair_variations = generate_pair_variations(signs_array, pos1, pos2)\n",
    "        all_variations.extend(pair_variations)\n",
    "    \n",
    "    return all_variations\n",
    "\n",
    "def generate_adjacent_sign_variations(signs_array: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate sign variations by sliding a two-position window across the array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signs_array : numpy.ndarray\n",
    "        Original array of signs (typically containing 1 or -1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of all possible sign variation arrays\n",
    "    \"\"\"\n",
    "    # Total length of the array\n",
    "    n = len(signs_array)\n",
    "    \n",
    "    # Will store all variations\n",
    "    all_variations = []\n",
    "    \n",
    "    # Iterate through adjacent position pairs \n",
    "    # (0,1), (1,2), (2,3), ... until the second-to-last pair\n",
    "    for pos1 in range(1, n - size + 1):\n",
    "        pos = [pos1 + i for i in range(size)]\n",
    "        \n",
    "        # Generate variations for this pair of adjacent positions\n",
    "        pair_variations = generate_pair_variations(signs_array, pos)\n",
    " \n",
    "        all_variations.extend(pair_variations)\n",
    "    \n",
    "    return all_variations\n",
    "\n",
    "def generate_pair_variations(signs_array: np.ndarray, pos: List[int]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate sign variations for a specific pair of positions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signs_array : numpy.ndarray\n",
    "        Original array of signs\n",
    "    pos1 : int\n",
    "        First position to modify\n",
    "    pos2 : int\n",
    "        Second position to modify\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[numpy.ndarray]\n",
    "        List of sign variation arrays\n",
    "    \"\"\"\n",
    "    # Validate input positions\n",
    "    # if pos1 < 0 or pos2 < 0 or pos1 >= len(signs_array) or pos2 >= len(signs_array):\n",
    "    #     raise ValueError(\"Positions must be within the array bounds\")\n",
    "    \n",
    "    # Generate all possible sign combinations for the two positions\n",
    "    sign_combinations = list(itertools.product([-1, 1], repeat=len(pos)))\n",
    "    \n",
    "    # Create variations\n",
    "    variations = []\n",
    "    for combo in sign_combinations:\n",
    "        # Create a copy of the original array\n",
    "        variation = signs_array.copy()\n",
    "        \n",
    "        # Modify the two specified positions\n",
    "        for i in range(len(combo)):\n",
    "            variation[pos[i]] *= combo[i]\n",
    "        \n",
    "        variations.append(tuple(variation))\n",
    "    \n",
    "    return variations\n",
    "\n",
    "\n",
    "# Example array\n",
    "signs = np.array([1]*4)\n",
    "\n",
    "# Generate variations for all position pairs\n",
    "all_variations = generate_adjacent_sign_variations(signs, 3)\n",
    "\n",
    "# print(\"Original array:\", signs)\n",
    "# print(f\"\\nTotal variations: {len(all_variations)}\")\n",
    "# print(f'4*len(signs)^2: {4*len(signs)**2}')\n",
    "# print(f'2^len(signs): {2**len(signs)}')\n",
    "\n",
    "print(\"\\nFirst few variations:\")\n",
    "for var in all_variations:  # Print first 10 variations\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(lp, cos_signal, abs_sin, ula_signal, esprit):   \n",
    "    signal = cos_signal + 1.0j * lp * abs_sin\n",
    "    R = ula_signal.get_cov_matrix_toeplitz(signal)\n",
    "    _, _ = esprit.estimate_theta_toeplitz(R)\n",
    "    eigs = np.abs(esprit.eigs)[:2]\n",
    "    obj = eigs[1] - eigs[0]\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the distribution of the signs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "narray = [2,2,2,2,2,3]\n",
    "\n",
    "def get_heavy_signs(narray, steps):\n",
    "\n",
    "    thetas = np.linspace(np.arcsin(0.09), np.arcsin(0.91), steps)\n",
    "    avals = np.sin(thetas)\n",
    "\n",
    "    ula_signal = TwoqULASignal(M=narray, C=3)\n",
    "\n",
    "    num_mc = 1000\n",
    "\n",
    "    sign_distributions = {}\n",
    "\n",
    "    for a,theta in zip(avals,thetas):\n",
    "        # theta = np.arcsin(a)\n",
    "        distr = {}\n",
    "        for i in range(num_mc):\n",
    "            signal, _ = estimate_signal(ula_signal.depths, ula_signal.n_samples, theta)\n",
    "                # print(measurements)\n",
    "            signs = tuple(np.sign(np.imag(signal)))\n",
    "            # print(signs)\n",
    "\n",
    "            distr[signs] = distr.get(signs, 0.0) + 1/num_mc\n",
    "        sign_distributions[str(a)] = distr\n",
    "\n",
    "    def get_signs(sign_distribution):\n",
    "        # Sort the dictionary by values in descending order\n",
    "        sorted_data = dict(sorted(sign_distribution.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "        # Create a new dictionary with cumulative sum close to 0.68\n",
    "        cumulative_dict = {}\n",
    "        cumulative_sum = 0.0\n",
    "\n",
    "        for key, value in sorted_data.items():\n",
    "            cumulative_dict[key] = value\n",
    "            if cumulative_sum + value > 0.9:\n",
    "                break\n",
    "            cumulative_sum += value\n",
    "\n",
    "        # Output the result\n",
    "        return cumulative_dict\n",
    "    \n",
    "    ret_dict = {}\n",
    "    for a in avals:\n",
    "        distr = get_signs(sign_distributions[str(a)])\n",
    "        ret_dict[str(a)] = distr\n",
    " \n",
    "    # Normalize the values\n",
    "    normalized_data = {}\n",
    "    for key, sub_dict in ret_dict.items():\n",
    "        total_sum = sum(sub_dict.values())\n",
    "        normalized_data[key] = {k: v / total_sum for k, v in sub_dict.items()}\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "normalized_data = get_heavy_signs(narray, 10)\n",
    "\n",
    "# normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Taking a random sample based on the normalized probabilities\n",
    "def sample_from_normalized(data, sample_size=1):\n",
    "    keys = list(map(str, list(data.keys())))\n",
    "    probabilities = list(data.values())\n",
    "    sampled_keys = np.random.choice(a=keys, size=sample_size, p=probabilities)\n",
    "    return [ast.literal_eval(t) for t in sampled_keys]\n",
    "avals = list(normalized_data.keys())\n",
    "# Get one random sample\n",
    "sampled = sample_from_normalized(normalized_data[avals[0]], sample_size=3)\n",
    "# print(\"Sampled keys:\",sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the distribution of signs so we don't have to try all possible signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "correct signs: [ 1.  1.  1.  1. -1.  1.  1.  1.  1.]\n",
      "rough estimate a: 0.18910752115495127\n",
      "avals to use: ['0.13981296510949756', '0.15634532387578837', '0.1728339924311597', '0.18927436306884374', '0.20566184157877418', '0.22199184853142423']\n",
      "number of signs Hamming distance two: 131\n",
      "current objective: -111.34188994413876\n",
      "current best signs: (1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0)\n",
      "current objective: -308.8108115642312\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0)\n",
      "current objective: -338.48863204266365\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0)\n",
      "current objective: -397.4516598486228\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0)\n",
      "current objective: -7.1407537803791\n",
      "current best signs: (1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0)\n",
      "current objective: -14.37126797725773\n",
      "current best signs: (1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0)\n",
      "current objective: -119.19130475889435\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0)\n",
      "current objective: -397.4516598486223\n",
      "current best signs: (1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0)\n",
      "[ 1.  1.  1.  1. -1.  1.  1.  1.  1.]\n",
      "basin obj: -397.45165984862274\n",
      "true obj: -397.45165984862274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'theta_est': 0.10012026433053868,\n",
       " 'theta_est1': 0.10012026433053861,\n",
       " 'error': 4.692056561071689e-05,\n",
       " 'error1': 4.692056561078628e-05,\n",
       " 'queries': 2600,\n",
       " 'depth': 128,\n",
       " 'true_obj': -397.45165984862274,\n",
       " 'basin_obj': -397.45165984862274,\n",
       " 'x_star': array([ 1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.])}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "a = 0.1\n",
    "print(a)\n",
    "theta = np.arcsin(a)\n",
    "\n",
    "narray = [2,2,2,2,2,2,2,2]\n",
    "heavy_signs = get_heavy_signs(narray, len(narray)**2)\n",
    "\n",
    "ula_signal = TwoqULASignal(M=narray, C=5)\n",
    "esprit = ESPIRIT()\n",
    "\n",
    "def sample_signs(heavy_signs, sample_size=3):\n",
    "    \"\"\"\n",
    "    Sample a set of sign variations from a learned sign distribution.\n",
    "\n",
    "    Args:\n",
    "        heavy_signs (dict): A dictionary where the keys are strings representing float values,\n",
    "                           and the values are dictionaries with keys as sign vectors and values\n",
    "                           as their corresponding probabilities.\n",
    "        sample_size (int, optional): The number of sign variations to sample. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the same as the keys in `heavy_signs`,\n",
    "              and the values are lists of sampled sign variations.\n",
    "    \"\"\"\n",
    "    avals = list(heavy_signs.keys())\n",
    "    signs_to_try = {}\n",
    "    for a in avals:\n",
    "        signs_to_try[a] = list(set(sample_from_normalized(heavy_signs[a], sample_size=sample_size)))\n",
    "\n",
    "    return signs_to_try\n",
    "\n",
    "def avals_to_usef(a0, avals, L=3):\n",
    "    \"\"\"\n",
    "    Find the `L` values in `avals` that are closest to the given `a0` value.\n",
    "\n",
    "    Args:\n",
    "        a0 (float): The reference value to find the closest `L` values for.\n",
    "        avals (list): A list of float values representing the keys in `heavy_signs`.\n",
    "        L (int, optional): The number of closest values to return. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of string representations of the `L` closest values to `a0` in `avals`.\n",
    "    \"\"\"\n",
    "    avals = list(map(float, avals))\n",
    "\n",
    "    left = 0\n",
    "    right = len(avals)\n",
    "    \n",
    "    while left < right:\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if avals[mid] <= a0:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid\n",
    "\n",
    "    avals_to_use = list(map(str, avals[left-L: left+L]))\n",
    "    return avals_to_use\n",
    "\n",
    "def all_signs_to_try(avals_to_use, signs_to_try, adjacency=2):\n",
    "    \"\"\"\n",
    "    Generate all possible sign variations within a Hamming distance of 2 from the given `avals_to_use`.\n",
    "\n",
    "    Args:\n",
    "        avals_to_use (list): A list of string representations of float values.\n",
    "        signs_to_try (dict): A dictionary where the keys are string representations of float values,\n",
    "                            and the values are lists of sign variations.\n",
    "        adjacency (int, optional): The maximum Hamming distance to consider for the sign variations.\n",
    "                                  Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of all possible sign variations within the specified Hamming distance.\n",
    "    \"\"\"\n",
    "    all_signs = []\n",
    "    for a in avals_to_use:\n",
    "        for x in signs_to_try[a]:\n",
    "            hamming_distance_two_signs = generate_adjacent_sign_variations(np.array(x), adjacency)\n",
    "            all_signs.extend(hamming_distance_two_signs)\n",
    "    all_signs = list(set(all_signs))\n",
    "    \n",
    "    return all_signs\n",
    "\n",
    "def minimize_obj(all_signs, cos_signal, abs_sin, ula_signal, esprit, disp):\n",
    "    \"\"\"\n",
    "    Find the sign variation that minimizes the objective function.\n",
    "\n",
    "    Args:\n",
    "        all_signs (list): A list of all possible sign variations to consider.\n",
    "        cos_signal (numpy.ndarray): The real part of the estimated signal.\n",
    "        abs_sin (numpy.ndarray): The absolute value of the imaginary part of the estimated signal.\n",
    "        ula_signal (ULASignal): An instance of the ULASignal class, containing information about the signal.\n",
    "        esprit (ESPRIT): An instance of the ESPRIT class, used for estimating the signal parameters.\n",
    "        disp (bool): If True, prints the current objective and the current best sign variation.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The sign variation that minimizes the objective function.\n",
    "    \"\"\"\n",
    "    obj = 1.0\n",
    "    for x in all_signs:\n",
    "        curr_obj = objective_function(np.array(x), cos_signal, abs_sin, ula_signal, esprit)\n",
    "        if curr_obj < obj:\n",
    "            if disp:\n",
    "                print(f'current objective: {curr_obj}')\n",
    "                print(f'current best signs: {x}')\n",
    "            obj = curr_obj\n",
    "            x_star = np.array(x)\n",
    "\n",
    "    return x_star\n",
    "\n",
    "def csae_with_local_minimization(theta, ula_signal, esprit, heavy_signs, sample=False, correction=False, optimize=False, disp=False):\n",
    "    \"\"\"\n",
    "    Perform CSAE (Compressive Sensing Angle Estimation) with local minimization.\n",
    "\n",
    "    Args:\n",
    "        theta (float): The true angle of the signal.\n",
    "        ula_signal (ULASignal): An instance of the ULASignal class, containing information about the signal.\n",
    "        esprit (ESPRIT): An instance of the ESPRIT class, used for estimating the signal parameters.\n",
    "        heavy_signs (dict): A dictionary where the keys are strings representing float values,\n",
    "                           and the values are dictionaries with keys as sign vectors and values\n",
    "                           as their corresponding probabilities.\n",
    "        sample (bool, optional): If True, samples sign variations from the learned sign distribution.\n",
    "                                If False, uses the learned sign distribution directly. Defaults to False.\n",
    "        correction (bool, optional): If True, applies a correction to the estimated angle. Defaults to False.\n",
    "        optimize (bool, optional): If True, uses a sliding window approach to find the best sign variation.\n",
    "                                  If False, uses the learned sign distribution directly. Defaults to False.\n",
    "        disp (bool, optional): If True, prints additional information during the process. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the estimated angles, errors, number of queries, maximum depth,\n",
    "              and other relevant information.\n",
    "    \"\"\"\n",
    "\n",
    "    depths = ula_signal.depths\n",
    "    n_samples = ula_signal.n_samples\n",
    "    csignal, measurements = estimate_signal(depths, n_samples, theta)\n",
    "\n",
    "    cos_signal = np.real(csignal)\n",
    "\n",
    "    correct_signs = np.sign(np.imag(csignal))\n",
    "    if disp:\n",
    "        print(f'correct signs: {correct_signs}')\n",
    "    abs_sin = np.abs(np.imag(csignal))\n",
    "\n",
    "    obj = 1.0\n",
    "    if sample:\n",
    "        # step 1: sample signs from learned sign distribution\n",
    "        signs_to_try = sample_signs(heavy_signs=heavy_signs, sample_size=3)\n",
    "        avals = heavy_signs.keys()\n",
    "\n",
    "        if optimize:\n",
    "            # step 2: using rough estimate where the amplitude is, use the a-values around that estimate\n",
    "            a0 = np.sqrt(0.5 - 0.5 * cos_signal[0])\n",
    "            avals_to_use = avals_to_usef(a0, avals, L=3)\n",
    "\n",
    "            if disp:\n",
    "                print(f'rough estimate a: {a0}')\n",
    "                print(f'avals to use: {avals_to_use}')\n",
    "\n",
    "            # step 3: now vary the signs in a sliding window of size \"adjacency\"\n",
    "            all_signs = all_signs_to_try(avals_to_use, signs_to_try, adjacency=2)\n",
    "\n",
    "            if disp:\n",
    "                print(f'number of signs Hamming distance two: {len(all_signs)}')\n",
    "\n",
    "            # step 4: try all the signs pick the ones that minimize the objective function\n",
    "            x_star = minimize_obj(all_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "\n",
    "            # step 5 (optional): do one more sweep\n",
    "            hamming_distance_one_signs = list(set(generate_adjacent_sign_variations(x_star, 1)))\n",
    "            x_star = minimize_obj(hamming_distance_one_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "        \n",
    "        else:\n",
    "            # here we don't vary the signs using a sliding window, but directly use the learned sign distribution (poor performance)\n",
    "            all_signs = []\n",
    "            for a in avals:\n",
    "                all_signs.extend(heavy_signs[a])\n",
    "            all_signs = list(set(all_signs))\n",
    "            x_star = minimize_obj(all_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "\n",
    "    else:\n",
    "        avals = list(heavy_signs.keys())\n",
    "        signs_to_try = {}\n",
    "        for a in avals:\n",
    "            signs_to_try[a] = heavy_signs[a]\n",
    "\n",
    "        if optimize:\n",
    "            # step 2: using rough estimate where the amplitude is, use the a-values around that estimate\n",
    "            a0 = np.sqrt(0.5 - 0.5 * cos_signal[0])\n",
    "            avals_to_use = avals_to_usef(a0, avals, L=3)\n",
    "\n",
    "            if disp:\n",
    "                print(f'rough estimate a: {a0}')\n",
    "                print(f'avals to use: {avals_to_use}')\n",
    "\n",
    "            # step 3: now vary the signs in a sliding window of size \"adjacency\"\n",
    "            all_signs = all_signs_to_try(avals_to_use, signs_to_try, adjacency=2)\n",
    "\n",
    "            if disp:\n",
    "                print(f'number of signs Hamming distance two: {len(all_signs)}')\n",
    "\n",
    "            # step 4: try all the signs pick the ones that minimize the objective function\n",
    "            x_star = minimize_obj(all_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "\n",
    "            # step 5 (optional): do one more sweep\n",
    "            hamming_distance_one_signs = list(set(generate_adjacent_sign_variations(x_star, 1)))\n",
    "            x_star = minimize_obj(hamming_distance_one_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "        \n",
    "        else:\n",
    "            # here we don't vary the signs using a sliding window, but directly use the learned sign distribution (poor performance)\n",
    "            all_signs = []\n",
    "            for a in avals:\n",
    "                all_signs.extend(heavy_signs[a])\n",
    "            all_signs = list(set(all_signs))\n",
    "            x_star = minimize_obj(all_signs, cos_signal, abs_sin, ula_signal, esprit, disp)\n",
    "\n",
    "\n",
    "    # Optimization is done.\n",
    "\n",
    "    if disp:\n",
    "        print(x_star)\n",
    "        \n",
    "    signal = cos_signal + 1.0j * x_star * abs_sin\n",
    "    R = ula_signal.get_cov_matrix_toeplitz(signal)\n",
    "    theta_est, _ = esprit.estimate_theta_toeplitz(R)\n",
    "    theta_est = np.abs(theta_est)\n",
    "    eigs = np.abs(esprit.eigs)[:2]\n",
    "    basin_obj = eigs[1] - eigs[0]\n",
    "    if correction:\n",
    "        theta_est = apply_correction(ula_signal, measurements, theta_est, theta) #apply correction\n",
    "    if disp:\n",
    "        print(f'basin obj: {basin_obj}')\n",
    "\n",
    "    cR = ula_signal.get_cov_matrix_toeplitz(csignal)\n",
    "    theta_est1, _ = esprit.estimate_theta_toeplitz(cR)\n",
    "    eigs = np.abs(esprit.eigs)[:2]\n",
    "    true_obj = eigs[1] - eigs[0]\n",
    "    if disp:\n",
    "        print(f'true obj: {true_obj}')\n",
    "\n",
    "    # compute queries required\n",
    "    num_queries = np.sum(np.array(ula_signal.depths)*np.array(ula_signal.n_samples)) + ula_signal.n_samples[0]\n",
    "    max_single_query = np.max(ula_signal.depths)\n",
    "\n",
    "    ret_dict = {'theta_est': theta_est, 'theta_est1': theta_est1, \n",
    "                'error': np.abs(np.sin(theta)-np.sin(theta_est)), \n",
    "                'error1': np.abs(np.sin(theta)-np.sin(theta_est1)), \n",
    "                'queries': num_queries, 'depth': max_single_query,\n",
    "                'true_obj': true_obj, 'basin_obj': basin_obj,\n",
    "                'x_star': x_star}\n",
    "    \n",
    "    return ret_dict\n",
    "\n",
    "csae_with_local_minimization(theta, ula_signal, esprit, heavy_signs, sample=True, correction=True, optimize=True, disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some statistics now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning the distribution of signs...\n",
      "theta = 0.41111546403370536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 3.72703095250019, 0.178007448477621\n",
      "theta = 1.0363905664439828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.8161590689618894, 0.13450311971161263\n",
      "theta = 0.7554209219922302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.1752347358136146, 0.10389180827766517\n",
      "theta = 0.6174118623048501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 3.5998263158118093, 0.1719320031432506\n",
      "theta = 0.2267530819288086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 6.327986966976157, 0.30223221334811495\n",
      "theta = 0.2267332789613546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 17.32139995839163, 0.8272907442813914\n",
      "theta = 0.14699569205633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 64.48651938293816, 3.0799531645582405\n",
      "theta = 0.9156206760936844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.9315469216305914, 0.14001418133161034\n",
      "theta = 0.6198241234230385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 3.6967593506166554, 0.17656164062646712\n",
      "theta = 0.7294478190327937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 3.9156832406093436, 0.1870177070141776\n",
      "theta = 0.11673252381145406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 29.408885047238687, 1.4046034649427432\n",
      "theta = 1.0673557731834724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 2.9946329479237606, 0.14302724527397065\n",
      "theta = 0.8725241084844789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 3.3371018725582364, 0.1593839700326322\n",
      "theta = 0.2732593578259982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 12.512037103448261, 0.5975898318064841\n",
      "theta = 0.24799415401854524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth: 8.297542340867073, 0.39630052971305424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "avals = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# avals = [np.random.uniform(0.1, 0.9)]\n",
    "# avals = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# avals = [0.1, 0.2, 0.3]\n",
    "avals = [np.random.uniform(0.1, 0.9) for _ in range(15)]\n",
    "# narray = [2]*5 + [3]\n",
    "narray = [2]*6\n",
    "\n",
    "print('learning the distribution of signs...')\n",
    "heavy_signs = get_heavy_signs(narray, int(6*len(narray)))\n",
    "num_queries = np.zeros(len(avals), dtype=int)\n",
    "max_single_query = np.zeros(len(avals), dtype=int)\n",
    "\n",
    "num_mc=100\n",
    "thetas = np.zeros((len(avals), num_mc))\n",
    "errors = np.zeros((len(avals), num_mc))\n",
    "thetas1 = np.zeros((len(avals), num_mc))\n",
    "errors1 = np.zeros((len(avals), num_mc))\n",
    "\n",
    "basin_obj = np.zeros((len(avals), num_mc))\n",
    "true_obj = np.zeros((len(avals), num_mc))\n",
    "\n",
    "ula_signal = TwoqULASignal(M=narray, C=5)\n",
    "esprit = ESPIRIT()\n",
    "\n",
    "for j,a in enumerate(avals):\n",
    "    theta = np.arcsin(a)\n",
    "    print(f'theta = {theta}')\n",
    "    for i in tqdm(range(num_mc)):\n",
    "        res = csae_with_local_minimization(theta, ula_signal, esprit, heavy_signs, sample=True, correction=True, optimize=True, disp=False)\n",
    "        thetas[j][i] = res['theta_est']\n",
    "        errors[j][i] = res['error']\n",
    "\n",
    "        thetas1[j][i] = res['theta_est1']\n",
    "        errors1[j][i] = res['error1']\n",
    "\n",
    "        basin_obj[j][i] = res['basin_obj']\n",
    "        true_obj[j][i] = res['true_obj']\n",
    "    num_queries[j] = res['queries']\n",
    "    max_single_query[j] = res['depth']\n",
    "\n",
    "    print(f'constant factors query and depth: {np.percentile(errors[j], 95) * num_queries[j]}, {np.percentile(errors[j], 95) * max_single_query[j]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(avals)):\n",
    "    print(np.sum(np.where(np.array(basin_obj[i])/np.array(true_obj[i]) >= 0.99, 1, 0))/len(basin_obj[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.02497076, 1.        , 1.04609651, 1.        , 1.        ,\n",
       "        1.16423963, 1.11683879, 1.        , 1.21549878, 1.00545266,\n",
       "        1.        , 1.00587115, 1.        , 1.        , 1.01386831,\n",
       "        1.        , 1.        , 1.        , 1.15791901, 1.09110154,\n",
       "        1.        , 1.        , 1.06740551, 1.01965389, 1.00646104,\n",
       "        1.28687647, 1.        , 1.52416662, 1.04571905, 1.0610772 ,\n",
       "        1.        , 1.11935286, 1.        , 1.0100827 , 1.18268422,\n",
       "        1.02999208, 1.15919422, 1.02643683, 1.        , 1.        ,\n",
       "        1.        , 1.02617146, 1.        , 1.2230548 , 1.11724276,\n",
       "        1.        , 1.00443219, 1.02663556, 1.        , 1.0493558 ,\n",
       "        1.00366068, 1.07676022, 1.07986259, 1.07108175, 1.0474498 ,\n",
       "        1.01119834, 1.00893675, 1.14512875, 1.10661684, 1.        ,\n",
       "        1.01951679, 1.14077254, 1.        , 1.08806654, 1.1123385 ,\n",
       "        1.02276714, 1.        , 1.        , 1.        , 1.19599272,\n",
       "        1.        , 1.19277811, 1.        , 1.        , 1.06259699,\n",
       "        1.05918477, 1.01577358, 1.01637294, 1.15541012, 1.00335209,\n",
       "        1.        , 1.046691  , 1.0917504 , 1.        , 1.03706613,\n",
       "        1.14796185, 1.        , 1.70464228, 1.24978663, 1.        ,\n",
       "        1.        , 1.02876208, 1.        , 1.01844191, 1.20173681,\n",
       "        1.        , 1.01521209, 1.10347782, 1.04797621, 1.1229412 ]),\n",
       " 1.0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=6\n",
    "np.array(basin_obj[i])/np.array(true_obj[i]), np.sum(np.where(np.array(basin_obj[i])/np.array(true_obj[i]) >= 0.99, 1, 0))/len(basin_obj[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.14094327, 0.14210457, 0.24457584, 0.14602306, 0.14536724,\n",
       "        0.14308264, 0.146503  , 0.14655331, 0.14266459, 0.14333197,\n",
       "        0.14568156, 0.2404924 , 0.14866643, 0.1455585 , 0.24222038,\n",
       "        0.14951485, 0.14763524, 0.14828353, 0.14691832, 0.14739484,\n",
       "        0.14335949, 0.14508579, 0.24335739, 0.14311765, 0.14269354,\n",
       "        0.2390811 , 0.14848022, 0.14848611, 0.14319923, 0.24153758,\n",
       "        0.14173488, 0.13850548, 0.14380408, 0.14636911, 0.14619498,\n",
       "        0.12697084, 0.14815267, 0.1362355 , 0.15107397, 0.14654374,\n",
       "        0.14313246, 0.13757741, 0.14456808, 0.1392772 , 0.14617979,\n",
       "        0.15083628, 0.1427158 , 0.1240662 , 0.14541335, 0.14267239,\n",
       "        0.1225064 , 0.14628344, 0.12338441, 0.14737236, 0.13927985,\n",
       "        0.14324154, 0.14549842, 0.14776629, 0.14466761, 0.1488627 ,\n",
       "        0.14559947, 0.14314409, 0.1463547 , 0.14644242, 0.24336449,\n",
       "        0.24143969, 0.14057451, 0.14640159, 0.14919144, 0.1498014 ,\n",
       "        0.14791762, 0.12317957, 0.14626426, 0.14960045, 0.14645983,\n",
       "        0.24311082, 0.12313245, 0.24271242, 0.24277263, 0.2410755 ,\n",
       "        0.14809326, 0.14347179, 0.14615005, 0.14781685, 0.24246144,\n",
       "        0.13878302, 0.14838796, 0.15156586, 0.14029093, 0.14942424,\n",
       "        0.14968516, 0.12647135, 0.14975926, 0.1449906 , 0.23759046,\n",
       "        0.14897797, 0.14197841, 0.2404282 , 0.14623728, 0.13873878]),\n",
       " 0.14646688973455957)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=6\n",
    "np.sin(thetas[i]), avals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.04501352, 0.04538693, 0.07864871, 0.04664737, 0.04643636,\n",
       "        0.04570147, 0.0468018 , 0.04681799, 0.04556702, 0.04578166,\n",
       "        0.04653749, 0.07730891, 0.04749807, 0.0464979 , 0.0778757 ,\n",
       "        0.04777118, 0.04716617, 0.04737482, 0.04693545, 0.0470888 ,\n",
       "        0.04579051, 0.04634582, 0.07824878, 0.04571273, 0.04557633,\n",
       "        0.07684618, 0.04743813, 0.04744003, 0.04573897, 0.07765171,\n",
       "        0.04526805, 0.04422985, 0.04593351, 0.04675872, 0.04670269,\n",
       "        0.04052547, 0.0473327 , 0.04350038, 0.04827317, 0.04681491,\n",
       "        0.0457175 , 0.04393159, 0.04617927, 0.04447791, 0.0466978 ,\n",
       "        0.04819663, 0.04558349, 0.03959352, 0.0464512 , 0.04556953,\n",
       "        0.0390932 , 0.04673115, 0.03937482, 0.04708157, 0.04447876,\n",
       "        0.04575258, 0.04647857, 0.04720834, 0.04621129, 0.04756124,\n",
       "        0.04651108, 0.04572123, 0.04675408, 0.04678231, 0.07825112,\n",
       "        0.0776196 , 0.04489496, 0.04676917, 0.04766707, 0.04786343,\n",
       "        0.04725705, 0.03930911, 0.04672498, 0.04779874, 0.04678791,\n",
       "        0.07816787, 0.039294  , 0.07803714, 0.0780569 , 0.07750014,\n",
       "        0.04731358, 0.04582663, 0.04668823, 0.04722462, 0.07795479,\n",
       "        0.04431906, 0.04740843, 0.04843156, 0.04480379, 0.04774201,\n",
       "        0.04782601, 0.04036518, 0.04784986, 0.0463152 , 0.07635762,\n",
       "        0.04759835, 0.04534636, 0.07728786, 0.0467163 , 0.04430484]),\n",
       " 0.046790182007957946)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=6\n",
    "thetas[i]/np.pi, np.arcsin(avals[i])/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "\n",
    "# csignal, measurements = estimate_signal(ula_signal.depths, ula_signal.n_samples, np.arcsin(avals[i]))\n",
    "\n",
    "# plt.plot(measurements, 'r', label='measurements')\n",
    "# plt.plot(np.cos((2*ula_signal.depths + 1)*theta2)**2, 'b', label='theta2')\n",
    "# plt.plot(np.cos((2*ula_signal.depths + 1)*theta1)**2, 'k', label='theta1')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth (68%) for 0.39963209507789: 2.2796823127361607, 0.10888034926501065\n",
      "constant factors query and depth (68%) for 0.8605714451279329: 0.7321750165723332, 0.03496955303032039\n",
      "constant factors query and depth (68%) for 0.685595153449124: 1.2038127001567607, 0.05749553194778558\n",
      "constant factors query and depth (68%) for 0.5789267873576293: 1.6251469867587645, 0.07761896056161263\n",
      "constant factors query and depth (68%) for 0.22481491235394924: 2.5553260585007473, 0.12204542368958794\n",
      "constant factors query and depth (68%) for 0.22479561626896213: 2.028211079451353, 0.09686978289916909\n",
      "constant factors query and depth (68%) for 0.14646688973455957: 3.5073533240260346, 0.16751538264004942\n",
      "constant factors query and depth (68%) for 0.7929409166199481: 1.2533817969522738, 0.05986301119772054\n",
      "constant factors query and depth (68%) for 0.5808920093945671: 1.6365906952796971, 0.07816552574470195\n",
      "constant factors query and depth (68%) for 0.6664580622368363: 1.2027303726968457, 0.057443838695968745\n",
      "constant factors query and depth (68%) for 0.11646759543664197: 2.5399211899008654, 0.12130966877138462\n",
      "constant factors query and depth (68%) for 0.8759278817295955: 0.941809430881603, 0.044981942967479545\n",
      "constant factors query and depth (68%) for 0.7659541126403374: 1.477739364679716, 0.07057859652201628\n",
      "constant factors query and depth (68%) for 0.26987128854262094: 2.055893279929671, 0.09819191784738729\n",
      "constant factors query and depth (68%) for 0.24545997376568052: 1.5841046547207542, 0.07565872977770767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7749252175495722"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc = 68\n",
    "avg = 0.0\n",
    "for i in range(len(avals)):\n",
    "    avg += np.percentile(errors[i], perc) * num_queries[i]/len(avals)\n",
    "    print(f'constant factors query and depth ({perc}%) for {avals[i]}: {np.percentile(errors[i], perc) * num_queries[i]}, {np.percentile(errors[i], perc) * max_single_query[i]}')\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant factors query and depth with known signs for 0.39963209507789: 4.314108491844801, 0.20604697274482633\n",
      "constant factors query and depth with known signs for 0.8605714451279329: 1.9583743787100234, 0.09353429868465783\n",
      "constant factors query and depth with known signs for 0.685595153449124: 2.378280610609581, 0.1135895217007561\n",
      "constant factors query and depth with known signs for 0.5789267873576293: 3.2963784252355137, 0.1574389695634872\n",
      "constant factors query and depth with known signs for 0.22481491235394924: 4.486854440469963, 0.2142975255149833\n",
      "constant factors query and depth with known signs for 0.22479561626896213: 3.869096446437671, 0.18479266609851563\n",
      "constant factors query and depth with known signs for 0.14646688973455957: 4.175131427288325, 0.1994092621988454\n",
      "constant factors query and depth with known signs for 0.7929409166199481: 2.1376129807041218, 0.10209494833213716\n",
      "constant factors query and depth with known signs for 0.5808920093945671: 3.1774890203211665, 0.1517606696272796\n",
      "constant factors query and depth with known signs for 0.6664580622368363: 2.2778208275561282, 0.10879144251014344\n",
      "constant factors query and depth with known signs for 0.11646759543664197: 3.422484234024073, 0.16346193356532884\n",
      "constant factors query and depth with known signs for 0.8759278817295955: 1.7626657864876982, 0.08418702263821842\n",
      "constant factors query and depth with known signs for 0.7659541126403374: 2.7620404783960404, 0.13191835120697507\n",
      "constant factors query and depth with known signs for 0.26987128854262094: 3.87938704058469, 0.18528415716225385\n",
      "constant factors query and depth with known signs for 0.24545997376568052: 2.703856667026251, 0.12913942290274633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.1067720837130697"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc = 68\n",
    "avg = 0.0\n",
    "for j in range(len(avals)):\n",
    "    avg += 2*np.percentile(errors1[j], perc) * num_queries[j]/len(avals)\n",
    "    print(f'constant factors query and depth with known signs for {avals[j]}: {2*np.percentile(errors1[j], perc) * num_queries[j]}, {2*np.percentile(errors1[j], perc) * max_single_query[j]}')\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
